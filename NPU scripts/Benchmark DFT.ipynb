{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World Example\n",
    "\n",
    "This is a simple Jupyter Notebook that walks through the 4 steps of compiling and running a PyTorch model on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:\n",
    "\n",
    "1. Get model\n",
    "2. Export to ONNX\n",
    "3. Quantize\n",
    "4. Run Model on CPU and IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the PyTorch library to define and instantiate a simple neural network model called `SmallModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "os.environ['NUM_OF_DPU_RUNNERS'] = \"4\"\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def generate_win_coeffs(M, P, window_fn=\"hamming\"):\n",
    "    win_coeffs = scipy.signal.get_window(window_fn, M*P)\n",
    "    sinc       = scipy.signal.firwin(M * P, cutoff=1.0/P, window=\"rectangular\")\n",
    "    win_coeffs *= sinc\n",
    "    return win_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFTLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFTLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FFTconv = nn.Conv2d(input_size, (input_size * 2), kernel_size=(1, 1), bias=False)\n",
    "        #F = torch.zeros((input_size, input_size), dtype=torch.complex128)\n",
    "\n",
    "\n",
    "        F = torch.from_numpy(np.fft.fft(np.eye(self.input_size)))\n",
    "        self.FFTconv.weight.data[0:self.input_size ,:,:] = torch.unsqueeze(torch.unsqueeze(F.real.float(), -1), -1)\n",
    "        self.FFTconv.weight.data[self.input_size:(self.input_size *2),:,:] = torch.unsqueeze(torch.unsqueeze(F.imag.float(), -1), -1)\n",
    "        #self.FFTconv.weight.requires_grad = False  # Set to `True` if you want to fine-tune the weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.FFTconv(x)\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "class PFB_FIR(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        for param in self.FIR.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        out = self.FIR(input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PFB_FIR_FFT(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR_FFT, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FFTlayer = FFTLayer(input_size=self.Maxsize)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        #for param in self.FIR.parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        input = self.FIR(input)\n",
    "        out = self.FFTlayer(input.view(input.shape[0], self.Maxsize, 1, self.P))\n",
    "        return out\n",
    "\n",
    "\n",
    "class slidingwindow_layer(nn.Module):\n",
    "  def __init__(self,size, stride, padding ) -> None:\n",
    "    super(slidingwindow_layer, self).__init__()\n",
    "    self.size = size\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.Slidwind = nn.Conv2d(in_channels=1, out_channels=self.size, kernel_size= (1, self.size), stride=(1,self.stride), padding=(self.padding,self.padding), bias=False, groups = 1)\n",
    "    with torch.no_grad():\n",
    "        # Set the weights to mimic the identity operation\n",
    "        identity_filter = torch.eye(self.size).reshape(self.size,1, 1,  self.size )\n",
    "        self.Slidwind.weight[:] = identity_filter\n",
    "\n",
    "  def forward(self, input):\n",
    "    slidwind_output = self.Slidwind(input)\n",
    "\n",
    "    return slidwind_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(size):\n",
    "    x = np.sin(np.arange(0, size) / np.pi)\n",
    "    #win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(1,*x.shape, 1,1)\n",
    "    PFB_layer = FFTLayer(input_size = size)\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/FFT.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/FFT.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/FFT_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runIPU(input_data):\n",
    "    # Compile and run\n",
    "\n",
    "    # Point to the config file path used for the VitisAI Execution Provider\n",
    "    config_file_path = \"vaip_config.json\"\n",
    "    \n",
    "    #aie_options = onnxruntime.SessionOptions()\n",
    "    #aie_options.enable_profiling = True\n",
    "    \n",
    "    aie_session = onnxruntime.InferenceSession(\n",
    "        \"models/FFT_quantized.onnx\",\n",
    "        providers = ['VitisAIExecutionProvider'],\n",
    "        sess_options=aie_options,\n",
    "        provider_options=[{'config_file': config_file_path}]\n",
    "    )\n",
    "    \n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    start = timer()\n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    aie_total = timer() - start\n",
    "    \n",
    "    #aie_session.end_profiling()\n",
    "    return aie_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfb_fir_frontend_FFT(x, win_coeffs, M, P):\n",
    "    #print(\"it X\", x.shape)\n",
    "    W = int(x.shape[0] / M / P)\n",
    "    x_p = x.reshape((W*M, P)).T\n",
    "    h_p = win_coeffs.reshape((M, P)).T\n",
    "    x_summed = np.zeros((P, M * W - M))\n",
    "    for t in range(0, M*W-M):\n",
    "        x_weighted = x_p[:, t:t+M] * h_p\n",
    "        x_summed[:, t] = x_weighted.sum(axis=1)\n",
    "    return np.fft.fft(x_summed.T,  axis=1)\n",
    "\n",
    "def numpy_unfold(vector, size = 3):\n",
    "  output = np.ones((vector.size - size +1, size))\n",
    "\n",
    "  for i in range(vector.size - size +1):\n",
    "    for j in range(size):\n",
    "      output[i, j] = vector[i + j]\n",
    "  return output\n",
    "\n",
    "def numpy_fft(vector):\n",
    "  return np.fft.fft(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def benchmarkCPU(size):\n",
    "    #print(\"M = \", M, \" P = \", P)\n",
    "    # Create some random input data for testing\n",
    "    input_data_test = np.random.uniform(low=-1, high=1, size=[size ]).astype(np.float32)\n",
    "    \n",
    "    #cpu_options = onnxruntime.SessionOptions()\n",
    "    #cpu_options.enable_profiling = True\n",
    "    \n",
    "\n",
    "    start = timer()\n",
    "    output = numpy_fft(vector = input_data_test)\n",
    "    cpu_total = timer() - start\n",
    "\n",
    "    return cpu_total \n",
    "    \n",
    "    #cpu_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparedspeedup(time1, time2):\n",
    "  if(time1<= time2):\n",
    "    return time1\n",
    "  else:\n",
    "    return time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.7633e-01,  2.7425e-01,  2.8344e-01,  ...,  2.8863e-01,\n",
      "            2.9108e-01,  2.7933e-01]],\n",
      "\n",
      "         [[-5.5454e-03,  5.4022e-03, -3.5508e-03,  ...,  1.4277e-02,\n",
      "            1.8669e-03,  9.3505e-03]],\n",
      "\n",
      "         [[-2.2321e-02,  5.2899e-03,  2.5479e-04,  ..., -8.9282e-03,\n",
      "            1.7856e-02, -8.3261e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5824e-03,  6.8331e-03, -2.0643e-03,  ...,  1.4411e-02,\n",
      "            4.1517e-03, -1.6858e-02]],\n",
      "\n",
      "         [[ 6.6560e-03,  1.0887e-02, -3.9439e-03,  ...,  9.0462e-03,\n",
      "            1.9658e-02,  4.2853e-03]],\n",
      "\n",
      "         [[-1.0377e-02, -8.6404e-03, -6.6723e-03,  ..., -2.3977e-03,\n",
      "           -1.0604e-02,  1.6293e-02]]]], grad_fn=<ConvolutionBackward0>)\n",
      "PFB_FIR_FFT(\n",
      "  (FIR): Conv2d(256, 256, kernel_size=(1, 16), stride=(1, 1), groups=256, bias=False)\n",
      "  (FFTlayer): FFTLayer(\n",
      "    (FFTconv): Conv2d(144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "M_increment = 1\n",
    "P_increment = 16\n",
    "Min_M = 1\n",
    "Min_P = 16\n",
    "Max_M = 30\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "M = 16\n",
    "P = 256\n",
    "\n",
    "x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "win_coeffs = torch.from_numpy(win_coeffs)\n",
    "\n",
    "# Timing pfb_fir_frontend_TINA_FFT\n",
    "xinput = np.random.rand(batchsize,*x.shape)\n",
    "PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "PFB_layer = PFB_layer.float()\n",
    "xinput = torch.from_numpy(xinput).float()\n",
    "output = PFB_layer(xinput)\n",
    "\n",
    "print(output)\n",
    "print(PFB_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to ONNX\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:42:23.826632\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1074.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/pfb_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import vai_q_onnx\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    ")\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_10-42-34.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Specify the path to the quantized ONNZ Model\n",
    "onnx_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "# Create some random input data for testing\n",
    "input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "cpu_options.enable_profiling = True\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "\n",
    "start = timer()\n",
    "cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "cpu_total = timer() - start\n",
    "\n",
    "cpu_session.end_profiling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_10-42-35.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and run\n",
    "\n",
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"vaip_config.json\"\n",
    "\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "aie_options.enable_profiling = True\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options=[{'config_file': config_file_path}]\n",
    ")\n",
    "ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "\n",
    "start = timer()\n",
    "ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "aie_total = timer() - start\n",
    "\n",
    "aie_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryzen Results: [array([[[[ 0.0234375 ,  0.        ,  0.01171875, ..., -0.0234375 ,\n",
      "           0.01171875, -0.01171875]],\n",
      "\n",
      "        [[-0.01953125, -0.02734375, -0.00390625, ..., -0.00390625,\n",
      "           0.015625  ,  0.03515625]],\n",
      "\n",
      "        [[ 0.01171875,  0.015625  ,  0.        , ...,  0.01171875,\n",
      "           0.0078125 , -0.015625  ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.03125   ,  0.0234375 ,  0.01953125, ..., -0.00390625,\n",
      "           0.01171875,  0.0078125 ]],\n",
      "\n",
      "        [[-0.015625  , -0.01171875, -0.0390625 , ...,  0.02734375,\n",
      "           0.02734375, -0.0390625 ]],\n",
      "\n",
      "        [[-0.03125   , -0.02734375, -0.0078125 , ...,  0.01171875,\n",
      "           0.02734375,  0.03515625]]]], dtype=float32)]\n",
      "CPU Results: [array([[[[ 0.0234375 ,  0.        ,  0.01171875, ..., -0.0234375 ,\n",
      "           0.01171875, -0.01171875]],\n",
      "\n",
      "        [[-0.01953125, -0.02734375, -0.00390625, ..., -0.00390625,\n",
      "           0.015625  ,  0.03515625]],\n",
      "\n",
      "        [[ 0.01171875,  0.015625  ,  0.        , ...,  0.01171875,\n",
      "           0.0078125 , -0.015625  ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.03125   ,  0.0234375 ,  0.01953125, ..., -0.00390625,\n",
      "           0.01171875,  0.0078125 ]],\n",
      "\n",
      "        [[-0.015625  , -0.01171875, -0.0390625 , ...,  0.02734375,\n",
      "           0.02734375, -0.0390625 ]],\n",
      "\n",
      "        [[-0.03125   , -0.02734375, -0.0078125 , ...,  0.01171875,\n",
      "           0.02734375,  0.03515625]]]], dtype=float32)]\n",
      "CPU Total Time: 0.0004456000000061522\n",
      "IPU Total Time: 0.0005136999999990621\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ryzen Results: {ryzen_outputs}\")\n",
    "print(f\"CPU Results: {cpu_results}\")\n",
    "\n",
    "print(f\"CPU Total Time: {cpu_total}\")\n",
    "print(f\"IPU Total Time: {aie_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 512, 1, 1] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 512\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:42:40.255412\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FFT.onnx\n",
      "                                  model_output --- models/FFT_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1335.77tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FFT_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FFT_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1024, 1, 1] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 1024\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:42:43.084863\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FFT.onnx\n",
      "                                  model_output --- models/FFT_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FFT_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FFT_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 2048\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:42:47.840064\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FFT.onnx\n",
      "                                  model_output --- models/FFT_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 2048, 1, 1] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1999.67tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "INFO:vai_q_onnx.refine:Shift cut of layer FFTconv.weight_DequantizeLinear_Output is 17. It exceeds range [0, 16]. Modify wpos from 7 to 6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FFT_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FFT_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:42:57.721549\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FFT.onnx\n",
      "                                  model_output --- models/FFT_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 4096, 1, 1] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2002.05tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "INFO:vai_q_onnx.refine:Shift cut of layer FFTconv.weight_DequantizeLinear_Output is 18. It exceeds range [0, 16]. Modify wpos from 7 to 5.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FFT_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FFT_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can create InferenceSession successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 10:43:25.328837\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FFT.onnx\n",
      "                                  model_output --- models/FFT_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 8192, 1, 1] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FFT.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2000.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "INFO:vai_q_onnx.refine:Shift cut of layer FFTconv.weight_DequantizeLinear_Output is 19. It exceeds range [0, 16]. Modify wpos from 7 to 4.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FFT_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FFT_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "Range_increment = 1024\n",
    "Min_Range = 512\n",
    "Max_Range = 8192\n",
    "base = 2\n",
    "Maxloop = 50\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Running on device: {device}\")\n",
    "\n",
    "current_range = Min_Range\n",
    "while current_range <= Max_Range:\n",
    "    #print(\"This is P: \", P)\n",
    "    print(\"current range\", current_range)\n",
    "    x = np.random.rand(current_range).astype(np.float32)\n",
    "    #y = np.random.rand(current_range,current_range).astype(np.float32)\n",
    "\n",
    "    # Converting NumPy arrays to CuPy arrays\n",
    "    elapsed_time_FFT = 0\n",
    "    elapsed_time_TINA_FFT_32_bit = 0\n",
    "    \n",
    "\n",
    "    througput_FFT = 0\n",
    "    througput_TINA_FFT_32_bit = 0\n",
    "    createmodel(current_range)\n",
    "\n",
    "\n",
    "    bytes_used = x.nbytes\n",
    "    for i in range(Maxloop):\n",
    "        x = np.random.rand(*x.shape).astype(np.float32)\n",
    "        input_data = np.random.uniform(low=-1, high=1, size=[1, current_range, 1, 1]).astype(np.float32)\n",
    "        #print(x.shape)\n",
    "        #y = np.random.rand(*y.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Timing pfb_fir_frontend_FFT\n",
    "        elapsed_time_FFT = benchmarkCPU(size = current_range)\n",
    "\n",
    "\n",
    "        \n",
    "        # Timing pfb_fir_frontend_TINA_elementwise_mult\n",
    "        #torch_y = torch.from_numpy(y).to(device).float()\n",
    "        #torch_y = torch_y.view( current_range, current_range)\n",
    "        \n",
    "        elapsed_time_TINA_FFT_32_bit = runIPU(input_data = input_data)\n",
    "        \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        are_results_equal = check_results(result_np, result_cp, resultTINA_32bit_np, resultTINA_16bit_np, resultjax_np)\n",
    "\n",
    "        if are_results_equal is False:\n",
    "          print(\"results are not equal\")\n",
    "          print(\"original: \", result_np )\n",
    "          print(\"cupy: \", result_cp)\n",
    "          print(\"TINA 32bit: \", resultTINA_32bit_np)\n",
    "          print(\"TINA 16bit: \", resultTINA_16bit_np)\n",
    "          print(\"JAX: \", resultjax_np)\n",
    "          \"\"\"\n",
    "\n",
    "\n",
    "        # Calculate speedup for CuPy, Torch, and JAX compared to NumPy\n",
    "        speedup_TINA_FFT_32_bit = elapsed_time_FFT / elapsed_time_TINA_FFT_32_bit\n",
    "        \n",
    "\n",
    "        # Average the elapsed times\n",
    "        throughput_FFT = elapsed_time_FFT/bytes_used\n",
    "        throughput_TINA_FFT_32_bit = elapsed_time_TINA_FFT_32_bit/bytes_used\n",
    "        #print(\"TINA: \", elapsed_time_TINA_FFT)\n",
    "        #print(\"cupy: \", elapsed_time_cp_FFT)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "        'elapsed_time_Multiplication_numpy': elapsed_time_FFT,\n",
    "        'elapsed_time_TINA_Multiplication_32_bit': elapsed_time_TINA_FFT_32_bit,\n",
    "        'speedup_TINA_Multiplication_32_bit': speedup_TINA_FFT_32_bit,\n",
    "        'throughput_Multiplication_FFT': throughput_FFT,\n",
    "        'throughput_TINA_Multiplication_32_bit': throughput_TINA_FFT_32_bit,\n",
    "        'bytes used': bytes_used\n",
    "        })\n",
    "\n",
    "\n",
    "    current_range *= base\n",
    "    current_range = int(current_range)\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Divide the values by Maxloop\n",
    "#df[['elapsed_time', 'elapsed_time_FFT', 'elapsed_time_cp', 'elapsed_time_cp_FFT', 'elapsed_time_TINA', 'elapsed_time_TINA_FFT']] /= Maxloop\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('output_DFT.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "M_increment = 10\n",
    "P_increment = 256\n",
    "Min_M = 1\n",
    "Min_P = 128\n",
    "Max_M = 31\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "for M in range(Min_M, Max_M + 1, M_increment):\n",
    "    for P in range(Min_P, Max_P + 1, P_increment):\n",
    "        createmodel(M, P)\n",
    "        #print(\"This is M again: \", M)\n",
    "        print(\"This is M: \", M, \"This is P: \", P)\n",
    "        x = np.sin((np.arange(0, M * P * 10) / np.pi),  dtype=np.float32)\n",
    "        win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "        # Converting NumPy arrays to CuPy arrays\n",
    "        \n",
    "        elapsed_time_FFT = 100\n",
    "        elapsed_time_TINA_FFT = 100\n",
    "        bytes_x = x.nbytes\n",
    "        coef_bytes = win_coeffs.nbytes\n",
    "        \n",
    "        for i in range(Maxloop):\n",
    "            x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "            input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            # Timing pfb_fir_frontend_FFT\n",
    "           \n",
    "            intermediatetime = benchmarkCPU( win_coeffs = win_coeffs, M = M, P = P)\n",
    "            \n",
    "            elapsed_time_FFT = comparedspeedup(elapsed_time_FFT, intermediatetime)\n",
    "            #print(\"elapsed_time_FFT = \" , elapsed_time_FFT)\n",
    "                        \n",
    "\n",
    "           \n",
    "            # Timing pfb_fir_frontend_TINA_FFT\n",
    "            xinput = np.random.rand(batchsize,*x.shape)\n",
    "            elapsed_time_TINA_FFT_temp = runIPU(input_data = input_data)\n",
    "            elapsed_time_TINA_FFT = comparedspeedup(elapsed_time_TINA_FFT, elapsed_time_TINA_FFT_temp)\n",
    "            #print(\"elapsed_time_TINA_FFT = \" , elapsed_time_TINA_FFT)\n",
    "            \n",
    "\n",
    "        # Average the elapsed times\n",
    "        \"\"\"\n",
    "        elapsed_time /= Maxloop\n",
    "        elapsed_time_FFT /= Maxloop\n",
    "        elapsed_time_cp /= Maxloop\n",
    "        elapsed_time_cp_FFT /= Maxloop\n",
    "        elapsed_time_TINA /= Maxloop\n",
    "        elapsed_time_TINA_FFT /= Maxloop\n",
    "        elapsed_time_jax /= Maxloop\n",
    "        elapsed_time_jax_fft /= Maxloop\n",
    "        \"\"\"\n",
    "\n",
    "         # Calculate speedup for CuPy, Torch, and JAX compared to NumPy\n",
    "        speedup_TINA_FFT = elapsed_time_FFT / elapsed_time_TINA_FFT\n",
    "        \n",
    "        #print(\"TINA: \", elapsed_time_TINA_FFT)\n",
    "        #print(\"cupy: \", elapsed_time_cp_FFT)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'Taps': M,\n",
    "            'Branches': P,\n",
    "            'bytes_x': bytes_x,\n",
    "            'coef_bytes': coef_bytes,\n",
    "            'elapsed_time_FFT_CPU': elapsed_time_FFT,\n",
    "            'elapsed_time_TINA_FFT': elapsed_time_TINA_FFT,\n",
    "            'speedup_TINA_FFT':speedup_TINA_FFT,\n",
    "            \n",
    "\n",
    "        })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Divide the values by Maxloop\n",
    "#df[['elapsed_time', 'elapsed_time_FFT', 'elapsed_time_cp', 'elapsed_time_cp_FFT', 'elapsed_time_TINA', 'elapsed_time_TINA_FFT']] /= Maxloop\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define model class\n",
    "class SmallModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Instantiate model and generate inputs\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "pytorch_model = SmallModel(input_size, output_size)\n",
    "\n",
    "print(pytorch_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The following code is used for exporting a PyTorch model (pytorch_model) to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the VitisAI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Prep for ONNX export\\ninputs = {\"x\": torch.rand(input_size, input_size)}\\ninput_names = [\\'input\\']\\noutput_names = [\\'output\\']\\ndynamic_axes = {\\'input\\': {0: \\'batch_size\\'}, \\'output\\': {0: \\'batch_size\\'}}\\ntmp_model_path = \"models/pfb.onnx\"\\n\\n# Call export function\\ntorch.onnx.export(\\n        pytorch_model,\\n        inputs,\\n        tmp_model_path,\\n        export_params=True,\\n        opset_version=13,  # Recommended opset\\n        input_names=input_names,\\n        output_names=output_names,\\n        dynamic_axes=dynamic_axes,\\n    )\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Prep for ONNX export\n",
    "inputs = {\"x\": torch.rand(input_size, input_size)}\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        pytorch_model,\n",
    "        inputs,\n",
    "        tmp_model_path,\n",
    "        export_paradef createmodel(M, P):\n",
    "    x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "    win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    win_coeffs = torch.from_numpy(win_coeffs)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(batchsize,*x.shape)\n",
    "    PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/pfb.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/pfb.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/pfb_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    \n",
    "ms=True,\n",
    "        opset_version=13,  # Recommended opset\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the Vitis AI Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. For more information on this quantization method, see [Vitis AI ONNX Quantization](https://ryzenai.docs.amd.com/en/latest/vai_quant/vai_q_onnx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Model\n",
    "\n",
    "#### CPU Run\n",
    "\n",
    "Before runnning the model on the IPU, we'll run the model on the CPU and get the execution time for comparison with the IPU. We'll also use the ONNX Runtime Profiling to get some more information about the inference. For more information on this, see [Profiling Tools](https://onnxruntime.ai/docs/performance/tune-performance/profiling-tools.html) from ONNX Runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPU Run\n",
    "\n",
    "Now, we'll run it on the IPU and time the execution so that we can compare the results with the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather our results and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
