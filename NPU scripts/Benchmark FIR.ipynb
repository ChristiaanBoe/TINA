{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World Example\n",
    "\n",
    "This is a simple Jupyter Notebook that walks through the 4 steps of compiling and running a PyTorch model on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:\n",
    "\n",
    "1. Get model\n",
    "2. Export to ONNX\n",
    "3. Quantize\n",
    "4. Run Model on CPU and IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the PyTorch library to define and instantiate a simple neural network model called `SmallModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "os.environ['NUM_OF_DPU_RUNNERS'] = \"4\"\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def generate_win_coeffs(M, P, window_fn=\"hamming\"):\n",
    "    win_coeffs = scipy.signal.get_window(window_fn, M*P)\n",
    "    sinc       = scipy.signal.firwin(M * P, cutoff=1.0/P, window=\"rectangular\")\n",
    "    win_coeffs *= sinc\n",
    "    return win_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch FIR layer\n",
    "class FIR_layer(nn.Module):\n",
    "    def __init__(self, win_coeffs):\n",
    "        super(FIR_layer, self).__init__()\n",
    "        self.win_coeffs = win_coeffs\n",
    "        self.size = self.win_coeffs.shape[0]\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "        self.FIR = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1, self.size), stride=(1, 1), padding=(0, self.size//2), bias=False, groups=1)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        for param in self.FIR.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.FIR(input)\n",
    "        return out\n",
    "\n",
    "class FFTLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFTLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FFTconv = nn.Conv2d(input_size, (input_size * 2), kernel_size=(1, 1), bias=False)\n",
    "        #F = torch.zeros((input_size, input_size), dtype=torch.complex128)\n",
    "\n",
    "\n",
    "        F = torch.from_numpy(np.fft.fft(np.eye(self.input_size)))\n",
    "        self.FFTconv.weight.data[0:self.input_size ,:,:] = torch.unsqueeze(torch.unsqueeze(F.real.float(), -1), -1)\n",
    "        self.FFTconv.weight.data[self.input_size:(self.input_size *2),:,:] = torch.unsqueeze(torch.unsqueeze(F.imag.float(), -1), -1)\n",
    "        #self.FFTconv.weight.requires_grad = False  # Set to `True` if you want to fine-tune the weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.FFTconv(x)\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "class PFB_FIR(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        for param in self.FIR.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        out = self.FIR(input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PFB_FIR_FFT(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR_FFT, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FFTlayer = FFTLayer(input_size=self.Maxsize)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        #for param in self.FIR.parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        input = self.FIR(input)\n",
    "        out = self.FFTlayer(input.view(input.shape[0], self.Maxsize, 1, self.P))\n",
    "        return out\n",
    "\n",
    "\n",
    "class slidingwindow_layer(nn.Module):\n",
    "  def __init__(self,size, stride, padding ) -> None:\n",
    "    super(slidingwindow_layer, self).__init__()\n",
    "    self.size = size\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.Slidwind = nn.Conv2d(in_channels=1, out_channels=self.size, kernel_size= (1, self.size), stride=(1,self.stride), padding=(self.padding,self.padding), bias=False, groups = 1)\n",
    "    with torch.no_grad():\n",
    "        # Set the weights to mimic the identity operation\n",
    "        identity_filter = torch.eye(self.size).reshape(self.size,1, 1,  self.size )\n",
    "        self.Slidwind.weight[:] = identity_filter\n",
    "\n",
    "  def forward(self, input):\n",
    "    slidwind_output = self.Slidwind(input)\n",
    "\n",
    "    return slidwind_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(size):\n",
    "    x = np.sin(np.arange(0, size * 10) / np.pi)\n",
    "    #xcoefs = np.sin(np.arange(0, size * 1000) / np.pi)\n",
    "\n",
    "    coeffs =  torch.from_numpy(np.random.rand(size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(1,1, 1,*x.shape)\n",
    "    PFB_layer = FIR_layer(win_coeffs = coeffs)\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/FIR.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/FIR.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/FIR_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runIPU(input_data):\n",
    "    # Compile and run\n",
    "\n",
    "    # Point to the config file path used for the VitisAI Execution Provider\n",
    "    config_file_path = \"vaip_config.json\"\n",
    "    \n",
    "    #aie_options = onnxruntime.SessionOptions()\n",
    "    #aie_options.enable_profiling = True\n",
    "    \n",
    "    aie_session = onnxruntime.InferenceSession(\n",
    "        \"models/FIR_quantized.onnx\",\n",
    "        providers = ['VitisAIExecutionProvider'],\n",
    "        sess_options=aie_options,\n",
    "        provider_options=[{'config_file': config_file_path}]\n",
    "    )\n",
    "    \n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    start = timer()\n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    aie_total = timer() - start\n",
    "    \n",
    "    #aie_session.end_profiling()\n",
    "    return aie_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfb_fir_frontend_FFT(x, win_coeffs, M, P):\n",
    "    #print(\"it X\", x.shape)\n",
    "    W = int(x.shape[0] / M / P)\n",
    "    x_p = x.reshape((W*M, P)).T\n",
    "    h_p = win_coeffs.reshape((M, P)).T\n",
    "    x_summed = np.zeros((P, M * W - M))\n",
    "    for t in range(0, M*W-M):\n",
    "        x_weighted = x_p[:, t:t+M] * h_p\n",
    "        x_summed[:, t] = x_weighted.sum(axis=1)\n",
    "    return np.fft.fft(x_summed.T,  axis=1)\n",
    "\n",
    "def numpy_unfold(vector, size = 3):\n",
    "  output = np.ones((vector.size - size +1, size))\n",
    "\n",
    "  for i in range(vector.size - size +1):\n",
    "    for j in range(size):\n",
    "      output[i, j] = vector[i + j]\n",
    "  return output\n",
    "\n",
    "# Numpy FIR filter implementation\n",
    "def numpy_fir_filter(signal, coefficients):\n",
    "    num_taps = len(coefficients)\n",
    "    signal_length = len(signal)\n",
    "    filtered_signal = np.zeros(signal_length)\n",
    "\n",
    "    for n in range(signal_length):\n",
    "        for k in range(num_taps):\n",
    "            if n - k >= 0:\n",
    "                filtered_signal[n] += coefficients[k] * signal[n - k]\n",
    "\n",
    "    return filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def benchmarkCPU(size):\n",
    "    #print(\"M = \", M, \" P = \", P)\n",
    "    # Create some random input data for testing\n",
    "    input_data_test = np.random.uniform(low=-1, high=1, size=[size * 10]).astype(np.float32)\n",
    "    coefficients = np.random.uniform(low=-1, high=1, size=[size]).astype(np.float32)\n",
    "    \n",
    "    #cpu_options = onnxruntime.SessionOptions()\n",
    "    #cpu_options.enable_profiling = True\n",
    "    \n",
    "\n",
    "    start = timer()\n",
    "    output = numpy_fir_filter(signal = input_data_test, coefficients = coefficients)\n",
    "    cpu_total = timer() - start\n",
    "\n",
    "    return cpu_total \n",
    "    \n",
    "    #cpu_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparedspeedup(time1, time2):\n",
    "  if(time1<= time2):\n",
    "    return time1\n",
    "  else:\n",
    "    return time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.9959e-01,  2.9181e-01,  2.8480e-01,  ...,  2.7345e-01,\n",
      "            2.7739e-01,  3.0410e-01]],\n",
      "\n",
      "         [[ 8.5603e-03,  1.0971e-02, -9.5091e-03,  ...,  8.1866e-03,\n",
      "            7.7269e-03,  3.2711e-03]],\n",
      "\n",
      "         [[ 7.6073e-03,  6.6400e-03, -4.5426e-03,  ...,  4.5335e-03,\n",
      "            5.7717e-03, -6.6657e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.0581e-03, -7.6584e-03,  3.5552e-03,  ...,  8.8564e-04,\n",
      "            7.9498e-03,  2.3655e-04]],\n",
      "\n",
      "         [[ 1.3244e-02,  3.9225e-03, -8.9702e-03,  ..., -7.1733e-03,\n",
      "           -8.8369e-03,  8.1739e-03]],\n",
      "\n",
      "         [[-3.2241e-03, -1.3147e-02,  1.4094e-03,  ...,  1.4664e-02,\n",
      "            1.5406e-02, -2.2268e-02]]]], grad_fn=<ConvolutionBackward0>)\n",
      "PFB_FIR_FFT(\n",
      "  (FIR): Conv2d(256, 256, kernel_size=(1, 16), stride=(1, 1), groups=256, bias=False)\n",
      "  (FFTlayer): FFTLayer(\n",
      "    (FFTconv): Conv2d(144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "M_increment = 1\n",
    "P_increment = 16\n",
    "Min_M = 1\n",
    "Min_P = 16\n",
    "Max_M = 30\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "M = 16\n",
    "P = 256\n",
    "\n",
    "x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "win_coeffs = torch.from_numpy(win_coeffs)\n",
    "\n",
    "# Timing pfb_fir_frontend_TINA_FFT\n",
    "xinput = np.random.rand(batchsize,*x.shape)\n",
    "PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "PFB_layer = PFB_layer.float()\n",
    "xinput = torch.from_numpy(xinput).float()\n",
    "output = PFB_layer(xinput)\n",
    "\n",
    "print(output)\n",
    "print(PFB_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to ONNX\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:01:21.009245\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 875.43tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/pfb_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import vai_q_onnx\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    ")\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_11-01-34.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Specify the path to the quantized ONNZ Model\n",
    "onnx_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "# Create some random input data for testing\n",
    "input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "cpu_options.enable_profiling = True\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "start = timer()\n",
    "cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "cpu_total = timer() - start\n",
    "\n",
    "cpu_session.end_profiling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_11-01-34.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and run\n",
    "\n",
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"vaip_config.json\"\n",
    "\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "aie_options.enable_profiling = True\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options=[{'config_file': config_file_path}]\n",
    ")\n",
    "\n",
    "start = timer()\n",
    "ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "aie_total = timer() - start\n",
    "\n",
    "aie_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryzen Results: [array([[[[ 0.015625  ,  0.04296875,  0.03515625, ..., -0.0078125 ,\n",
      "           0.0078125 ,  0.015625  ]],\n",
      "\n",
      "        [[-0.01171875,  0.        ,  0.        , ...,  0.01953125,\n",
      "           0.        , -0.01171875]],\n",
      "\n",
      "        [[-0.00390625,  0.00390625, -0.0234375 , ...,  0.0078125 ,\n",
      "           0.01171875,  0.03515625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0078125 , -0.01171875,  0.0234375 , ..., -0.01171875,\n",
      "           0.0390625 , -0.02734375]],\n",
      "\n",
      "        [[ 0.015625  ,  0.01171875,  0.015625  , ..., -0.00390625,\n",
      "           0.0078125 ,  0.00390625]],\n",
      "\n",
      "        [[-0.01953125,  0.01953125,  0.01171875, ...,  0.03125   ,\n",
      "           0.00390625, -0.0234375 ]]]], dtype=float32)]\n",
      "CPU Results: [array([[[[ 0.015625  ,  0.04296875,  0.03515625, ..., -0.0078125 ,\n",
      "           0.0078125 ,  0.015625  ]],\n",
      "\n",
      "        [[-0.01171875,  0.        ,  0.        , ...,  0.01953125,\n",
      "           0.        , -0.01171875]],\n",
      "\n",
      "        [[-0.00390625,  0.00390625, -0.0234375 , ...,  0.0078125 ,\n",
      "           0.01171875,  0.03515625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0078125 , -0.01171875,  0.0234375 , ..., -0.01171875,\n",
      "           0.0390625 , -0.02734375]],\n",
      "\n",
      "        [[ 0.015625  ,  0.01171875,  0.015625  , ..., -0.00390625,\n",
      "           0.0078125 ,  0.00390625]],\n",
      "\n",
      "        [[-0.01953125,  0.01953125,  0.01171875, ...,  0.03125   ,\n",
      "           0.00390625, -0.0234375 ]]]], dtype=float32)]\n",
      "CPU Total Time: 0.0006495999999742708\n",
      "IPU Total Time: 0.0013407999999799358\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ryzen Results: {ryzen_outputs}\")\n",
    "print(f\"CPU Results: {cpu_results}\")\n",
    "\n",
    "print(f\"CPU Total Time: {cpu_total}\")\n",
    "print(f\"IPU Total Time: {aie_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 80] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "current range 8\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:01:44.676953\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2000.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:01:47.259201 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:01:47.262203 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 160] type <class 'numpy.float32'> \n",
      "I0617 11:01:47.275204 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:01:47.279205 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:01:47.281669 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:01:47.284670 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:01:47.285670 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:01:47.299678 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:01:47.310667 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:01:47.321668 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:01:47.323668 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:01:47.323668 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 16\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:01:47.254201\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2000.14tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:01:47.332121 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:01:50.751309 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:01:50.753499 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 320] type <class 'numpy.float32'> \n",
      "I0617 11:01:50.764498 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:01:50.766581 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:01:50.768397 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:01:50.773402 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:01:50.774402 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:01:50.787686 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:01:50.798686 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:01:50.808686 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:01:50.811046 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:01:50.812045 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 32\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:01:50.747308\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2001.10tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:01:50.823511 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:01:55.091745 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:01:55.094748 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 640] type <class 'numpy.float32'> \n",
      "I0617 11:01:55.107745 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:01:55.110972 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:01:55.114250 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:01:55.117250 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:01:55.118249 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:01:55.133151 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:01:55.145153 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:01:55.158457 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:01:55.159458 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:01:55.160459 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 64\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:01:55.085748\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2001.58tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:01:55.168744 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:02:00.595561 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:02:00.597561 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 1280] type <class 'numpy.float32'> \n",
      "I0617 11:02:00.608561 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 1280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:02:00.612562 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:02:00.615564 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:02:00.618588 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:02:00.618588 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:02:00.630150 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:02:00.637154 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:02:00.646052 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:02:00.647051 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:02:00.647051 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 128\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:02:00.590560\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:02:00.653456 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:02:08.272836 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:02:08.276180 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 2560] type <class 'numpy.float32'> \n",
      "I0617 11:02:08.289181 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 2560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:02:08.293179 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:02:08.295182 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:02:08.298321 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:02:08.299319 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:02:08.311640 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:02:08.329760 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:02:08.348507 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:02:08.350507 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:02:08.350507 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 256\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:02:08.267835\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1999.19tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:02:08.361507 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:02:22.484720 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:02:22.488723 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 5120] type <class 'numpy.float32'> \n",
      "I0617 11:02:22.501043 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 5120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:02:22.503551 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:02:22.507553 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:02:22.510199 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:02:22.511205 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:02:22.519686 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:02:22.533200 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:02:22.546451 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:02:22.547451 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:02:22.548453 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 512\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:02:22.479720\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:02:22.555451 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:03:00.568744 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:03:00.570747 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 10240] type <class 'numpy.float32'> \n",
      "I0617 11:03:00.581743 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 10240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:03:00.584174 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:03:00.589219 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:03:00.592221 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:03:00.593219 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:03:00.603219 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:03:00.617220 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:03:00.628704 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:03:00.630706 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:03:00.630706 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 1024\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:03:00.563745\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1999.67tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:03:00.638840 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 11:05:11.703221 8696 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "I0617 11:05:11.706220 8696 quant_utils.py:1232] The input ONNX model models/FIR.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1, 1, 20480] type <class 'numpy.float32'> \n",
      "I0617 11:05:11.715627 8696 quant_utils.py:507] Random input name input shape [1, 1, 1, 20480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 11:05:11.717626 8696 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/FIR.onnx can run inference successfully\n",
      "I0617 11:05:11.729963 8696 quant_utils.py:1211] The input ONNX model models/FIR.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 11:05:11.733238 8696 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 11:05:11.733238 8696 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 11:05:11.753618 8696 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 11:05:11.766387 8696 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 11:05:11.783684 8696 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 11:05:11.786685 8696 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 11:05:11.787687 8696 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current range 2048\n",
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 11:05:11.698220\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/FIR.onnx\n",
      "                                  model_output --- models/FIR_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1998.24tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 11:05:11.799440 8696 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "INFO:vai_q_onnx.refine:Shift cut of layer FIR.weight_DequantizeLinear_Output is 17. It exceeds range [0, 16]. Modify wpos from 7 to 6.\n",
      "I0617 11:05:11.800439 8696 refine.py:210] Shift cut of layer FIR.weight_DequantizeLinear_Output is 17. It exceeds range [0, 16]. Modify wpos from 7 to 6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/FIR_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/FIR_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "Range_increment = 1024\n",
    "Min_Range = 8\n",
    "Max_Range = 2048\n",
    "base = 2\n",
    "Maxloop = 50\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "current_range = Min_Range\n",
    "while current_range <= Max_Range:\n",
    "    #print(\"This is P: \", P)\n",
    "    print(\"current range\", current_range)\n",
    "    x = np.random.rand(current_range).astype(np.float32)\n",
    "    #y = np.random.rand(current_range,current_range).astype(np.float32)\n",
    "\n",
    "    # Converting NumPy arrays to CuPy arrays\n",
    "    elapsed_time_FFT = 0\n",
    "    elapsed_time_TINA_FFT_32_bit = 0\n",
    "    \n",
    "\n",
    "    througput_FFT = 0\n",
    "    througput_TINA_FFT_32_bit = 0\n",
    "    createmodel(current_range)\n",
    "\n",
    "\n",
    "    bytes_used = x.nbytes\n",
    "    for i in range(Maxloop):\n",
    "        x = np.random.rand(*x.shape).astype(np.float32)\n",
    "        input_data = np.random.uniform(low=-1, high=1, size=[1, 1, 1, current_range * 10]).astype(np.float32)\n",
    "        #print(x.shape)\n",
    "        #y = np.random.rand(*y.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Timing pfb_fir_frontend_FFT\n",
    "        elapsed_time_FFT = benchmarkCPU(size = current_range)\n",
    "\n",
    "\n",
    "        \n",
    "        # Timing pfb_fir_frontend_TINA_elementwise_mult\n",
    "        #torch_y = torch.from_numpy(y).to(device).float()\n",
    "        #torch_y = torch_y.view( current_range, current_range)\n",
    "        \n",
    "        elapsed_time_TINA_FFT_32_bit = runIPU(input_data = input_data)\n",
    "        \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        are_results_equal = check_results(result_np, result_cp, resultTINA_32bit_np, resultTINA_16bit_np, resultjax_np)\n",
    "\n",
    "        if are_results_equal is False:\n",
    "          print(\"results are not equal\")\n",
    "          print(\"original: \", result_np )\n",
    "          print(\"cupy: \", result_cp)\n",
    "          print(\"TINA 32bit: \", resultTINA_32bit_np)\n",
    "          print(\"TINA 16bit: \", resultTINA_16bit_np)\n",
    "          print(\"JAX: \", resultjax_np)\n",
    "          \"\"\"\n",
    "\n",
    "\n",
    "        # Calculate speedup for CuPy, Torch, and JAX compared to NumPy\n",
    "        speedup_TINA_FFT_32_bit = elapsed_time_FFT / elapsed_time_TINA_FFT_32_bit\n",
    "        \n",
    "\n",
    "        # Average the elapsed times\n",
    "        throughput_FFT = elapsed_time_FFT/bytes_used\n",
    "        throughput_TINA_FFT_32_bit = elapsed_time_TINA_FFT_32_bit/bytes_used\n",
    "        #print(\"TINA: \", elapsed_time_TINA_FFT)\n",
    "        #print(\"cupy: \", elapsed_time_cp_FFT)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "        'elapsed_time_Multiplication_numpy': elapsed_time_FFT,\n",
    "        'elapsed_time_TINA_Multiplication_32_bit': elapsed_time_TINA_FFT_32_bit,\n",
    "        'speedup_TINA_Multiplication_32_bit': speedup_TINA_FFT_32_bit,\n",
    "        'throughput_Multiplication_FFT': throughput_FFT,\n",
    "        'throughput_TINA_Multiplication_32_bit': throughput_TINA_FFT_32_bit,\n",
    "        'bytes used': bytes_used\n",
    "        })\n",
    "\n",
    "\n",
    "    current_range *= base\n",
    "    current_range = int(current_range)\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Divide the values by Maxloop\n",
    "#df[['elapsed_time', 'elapsed_time_FFT', 'elapsed_time_cp', 'elapsed_time_cp_FFT', 'elapsed_time_TINA', 'elapsed_time_TINA_FFT']] /= Maxloop\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('Benchmarks_FIR.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "M_increment = 10\n",
    "P_increment = 256\n",
    "Min_M = 1\n",
    "Min_P = 128\n",
    "Max_M = 31\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "for M in range(Min_M, Max_M + 1, M_increment):\n",
    "    for P in range(Min_P, Max_P + 1, P_increment):\n",
    "        createmodel(M, P)\n",
    "        #print(\"This is M again: \", M)\n",
    "        print(\"This is M: \", M, \"This is P: \", P)\n",
    "        x = np.sin((np.arange(0, M * P * 10) / np.pi),  dtype=np.float32)\n",
    "        win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "        # Converting NumPy arrays to CuPy arrays\n",
    "        \n",
    "        elapsed_time_FFT = 100\n",
    "        elapsed_time_TINA_FFT = 100\n",
    "        bytes_x = x.nbytes\n",
    "        coef_bytes = win_coeffs.nbytes\n",
    "        \n",
    "        for i in range(Maxloop):\n",
    "            x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "            input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            # Timing pfb_fir_frontend_FFT\n",
    "           \n",
    "            intermediatetime = benchmarkCPU( win_coeffs = win_coeffs, M = M, P = P)\n",
    "            \n",
    "            elapsed_time_FFT = comparedspeedup(elapsed_time_FFT, intermediatetime)\n",
    "            #print(\"elapsed_time_FFT = \" , elapsed_time_FFT)\n",
    "                        \n",
    "\n",
    "           \n",
    "            # Timing pfb_fir_frontend_TINA_FFT\n",
    "            xinput = np.random.rand(batchsize,*x.shape)\n",
    "            elapsed_time_TINA_FFT_temp = runIPU(input_data = input_data)\n",
    "            elapsed_time_TINA_FFT = comparedspeedup(elapsed_time_TINA_FFT, elapsed_time_TINA_FFT_temp)\n",
    "            #print(\"elapsed_time_TINA_FFT = \" , elapsed_time_TINA_FFT)\n",
    "            \n",
    "\n",
    "        # Average the elapsed times\n",
    "        \"\"\"\n",
    "        elapsed_time /= Maxloop\n",
    "        elapsed_time_FFT /= Maxloop\n",
    "        elapsed_time_cp /= Maxloop\n",
    "        elapsed_time_cp_FFT /= Maxloop\n",
    "        elapsed_time_TINA /= Maxloop\n",
    "        elapsed_time_TINA_FFT /= Maxloop\n",
    "        elapsed_time_jax /= Maxloop\n",
    "        elapsed_time_jax_fft /= Maxloop\n",
    "        \"\"\"\n",
    "\n",
    "         # Calculate speedup for CuPy, Torch, and JAX compared to NumPy\n",
    "        speedup_TINA_FFT = elapsed_time_FFT / elapsed_time_TINA_FFT\n",
    "        \n",
    "        #print(\"TINA: \", elapsed_time_TINA_FFT)\n",
    "        #print(\"cupy: \", elapsed_time_cp_FFT)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'Taps': M,\n",
    "            'Branches': P,\n",
    "            'bytes_x': bytes_x,\n",
    "            'coef_bytes': coef_bytes,\n",
    "            'elapsed_time_FFT_CPU': elapsed_time_FFT,\n",
    "            'elapsed_time_TINA_FFT': elapsed_time_TINA_FFT,\n",
    "            'speedup_TINA_FFT':speedup_TINA_FFT,\n",
    "            \n",
    "\n",
    "        })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Divide the values by Maxloop\n",
    "#df[['elapsed_time', 'elapsed_time_FFT', 'elapsed_time_cp', 'elapsed_time_cp_FFT', 'elapsed_time_TINA', 'elapsed_time_TINA_FFT']] /= Maxloop\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define model class\n",
    "class SmallModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Instantiate model and generate inputs\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "pytorch_model = SmallModel(input_size, output_size)\n",
    "\n",
    "print(pytorch_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The following code is used for exporting a PyTorch model (pytorch_model) to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the VitisAI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Prep for ONNX export\\ninputs = {\"x\": torch.rand(input_size, input_size)}\\ninput_names = [\\'input\\']\\noutput_names = [\\'output\\']\\ndynamic_axes = {\\'input\\': {0: \\'batch_size\\'}, \\'output\\': {0: \\'batch_size\\'}}\\ntmp_model_path = \"models/pfb.onnx\"\\n\\n# Call export function\\ntorch.onnx.export(\\n        pytorch_model,\\n        inputs,\\n        tmp_model_path,\\n        export_params=True,\\n        opset_version=13,  # Recommended opset\\n        input_names=input_names,\\n        output_names=output_names,\\n        dynamic_axes=dynamic_axes,\\n    )\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Prep for ONNX export\n",
    "inputs = {\"x\": torch.rand(input_size, input_size)}\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        pytorch_model,\n",
    "        inputs,\n",
    "        tmp_model_path,\n",
    "        export_paradef createmodel(M, P):\n",
    "    x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "    win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    win_coeffs = torch.from_numpy(win_coeffs)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(batchsize,*x.shape)\n",
    "    PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/pfb.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/pfb.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/pfb_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    \n",
    "ms=True,\n",
    "        opset_version=13,  # Recommended opset\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the Vitis AI Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. For more information on this quantization method, see [Vitis AI ONNX Quantization](https://ryzenai.docs.amd.com/en/latest/vai_quant/vai_q_onnx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Model\n",
    "\n",
    "#### CPU Run\n",
    "\n",
    "Before runnning the model on the IPU, we'll run the model on the CPU and get the execution time for comparison with the IPU. We'll also use the ONNX Runtime Profiling to get some more information about the inference. For more information on this, see [Profiling Tools](https://onnxruntime.ai/docs/performance/tune-performance/profiling-tools.html) from ONNX Runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPU Run\n",
    "\n",
    "Now, we'll run it on the IPU and time the execution so that we can compare the results with the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather our results and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
