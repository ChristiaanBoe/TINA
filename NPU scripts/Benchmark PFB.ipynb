{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World Example\n",
    "\n",
    "This is a simple Jupyter Notebook that walks through the 4 steps of compiling and running a PyTorch model on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:\n",
    "\n",
    "1. Get model\n",
    "2. Export to ONNX\n",
    "3. Quantize\n",
    "4. Run Model on CPU and IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\chris\\anaconda3\\envs\\ryzentestweird\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the PyTorch library to define and instantiate a simple neural network model called `SmallModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "os.environ['NUM_OF_DPU_RUNNERS'] = \"4\"\n",
    "print(os.environ['NUM_OF_DPU_RUNNERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def generate_win_coeffs(M, P, window_fn=\"hamming\"):\n",
    "    win_coeffs = scipy.signal.get_window(window_fn, M*P)\n",
    "    sinc       = scipy.signal.firwin(M * P, cutoff=1.0/P, window=\"rectangular\")\n",
    "    win_coeffs *= sinc\n",
    "    return win_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class FFTLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFTLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FFTconv = nn.Conv2d(input_size, (input_size * 2), kernel_size=(1, 1), bias=False)\n",
    "        #F = torch.zeros((input_size, input_size), dtype=torch.complex128)\n",
    "\n",
    "\n",
    "        F = torch.from_numpy(np.fft.fft(np.eye(self.input_size)))\n",
    "        self.FFTconv.weight.data[0:self.input_size ,:,:] = torch.unsqueeze(torch.unsqueeze(F.real.float(), -1), -1)\n",
    "        self.FFTconv.weight.data[self.input_size:(self.input_size *2),:,:] = torch.unsqueeze(torch.unsqueeze(F.imag.float(), -1), -1)\n",
    "        #self.FFTconv.weight.requires_grad = False  # Set to `True` if you want to fine-tune the weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.FFTconv(x)\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "class PFB_FIR(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        for param in self.FIR.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        out = self.FIR(input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PFB_FIR_FFT(nn.Module):\n",
    "    def __init__(self, win_coeffs, M, P, expected_input_size):\n",
    "        super(PFB_FIR_FFT, self).__init__()\n",
    "        self.win_coeffs = win_coeffs.reshape((M, P)).T\n",
    "        self.win_coeffs = self.win_coeffs.unsqueeze(0).unsqueeze(1)\n",
    "        self.win_coeffs = self.win_coeffs.view(P, 1, 1, M)\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.size = expected_input_size\n",
    "        self.W = int(self.size / self.M / self.P)\n",
    "        self.Maxsize =  self.M * self.W - self.M\n",
    "        self.WM = self.M * self.W\n",
    "        self.FIR = nn.Conv2d(in_channels=self.P, out_channels=self.P, kernel_size=(1, self.M), stride=(1, 1), padding=(0, 0), bias=False, groups=self.P)\n",
    "        self.FFTlayer = FFTLayer(input_size=self.Maxsize)\n",
    "        self.FIR.weight = nn.Parameter(self.win_coeffs)\n",
    "        #for param in self.FIR.parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], self.WM, 1, self.P).permute(0, 3, 2, 1)[:, :, :, 0:self.WM-1]\n",
    "        input = self.FIR(input)\n",
    "        out = self.FFTlayer(input.view(input.shape[0], self.Maxsize, 1, self.P))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(M, P):\n",
    "    x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "    win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    win_coeffs = torch.from_numpy(win_coeffs)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(batchsize,*x.shape)\n",
    "    PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/pfb.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/pfb.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/pfb_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runIPU(input_data):\n",
    "    # Compile and run\n",
    "\n",
    "    # Point to the config file path used for the VitisAI Execution Provider\n",
    "    config_file_path = \"vaip_config.json\"\n",
    "    \n",
    "    #aie_options = onnxruntime.SessionOptions()\n",
    "    #aie_options.enable_profiling = True\n",
    "    \n",
    "    aie_session = onnxruntime.InferenceSession(\n",
    "        onnx_model_path,\n",
    "        providers = ['VitisAIExecutionProvider'],\n",
    "        sess_options=aie_options,\n",
    "        provider_options=[{'config_file': config_file_path}]\n",
    "    )\n",
    "    \n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    start = timer()\n",
    "    ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "    aie_total = timer() - start\n",
    "    \n",
    "    #aie_session.end_profiling()\n",
    "    return aie_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfb_fir_frontend_FFT(x, win_coeffs, M, P):\n",
    "    #print(\"it X\", x.shape)\n",
    "    W = int(x.shape[0] / M / P)\n",
    "    x_p = x.reshape((W*M, P)).T\n",
    "    h_p = win_coeffs.reshape((M, P)).T\n",
    "    x_summed = np.zeros((P, M * W - M))\n",
    "    for t in range(0, M*W-M):\n",
    "        x_weighted = x_p[:, t:t+M] * h_p\n",
    "        x_summed[:, t] = x_weighted.sum(axis=1)\n",
    "    return np.fft.fft(x_summed.T,  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def benchmarkCPU(M,P, win_coeffs):\n",
    "    #print(\"M = \", M, \" P = \", P)\n",
    "    # Create some random input data for testing\n",
    "    input_data_test = np.random.uniform(low=-1, high=1, size=[M * P * 10]).astype(np.float32)\n",
    "    \n",
    "    #cpu_options = onnxruntime.SessionOptions()\n",
    "    #cpu_options.enable_profiling = True\n",
    "    \n",
    "\n",
    "    start = timer()\n",
    "    output = pfb_fir_frontend_FFT(x = input_data_test, win_coeffs = win_coeffs, M = M, P = P)\n",
    "    cpu_total = timer() - start\n",
    "\n",
    "    return cpu_total \n",
    "    \n",
    "    #cpu_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparedspeedup(time1, time2):\n",
    "  if(time1<= time2):\n",
    "    return time1\n",
    "  else:\n",
    "    return time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2735,  0.3040,  0.2553,  ...,  0.2877,  0.2704,  0.2610]],\n",
      "\n",
      "         [[-0.0116,  0.0043, -0.0050,  ..., -0.0228,  0.0045,  0.0016]],\n",
      "\n",
      "         [[ 0.0053,  0.0054, -0.0027,  ...,  0.0122,  0.0165,  0.0135]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0039, -0.0139,  0.0008,  ...,  0.0071, -0.0109, -0.0027]],\n",
      "\n",
      "         [[ 0.0005,  0.0097,  0.0006,  ..., -0.0070,  0.0019,  0.0182]],\n",
      "\n",
      "         [[ 0.0090,  0.0067,  0.0030,  ...,  0.0185, -0.0052,  0.0020]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "PFB_FIR_FFT(\n",
      "  (FIR): Conv2d(256, 256, kernel_size=(1, 16), stride=(1, 1), groups=256, bias=False)\n",
      "  (FFTlayer): FFTLayer(\n",
      "    (FFTconv): Conv2d(144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "M_increment = 1\n",
    "P_increment = 16\n",
    "Min_M = 1\n",
    "Min_P = 16\n",
    "Max_M = 30\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "M = 16\n",
    "P = 256\n",
    "\n",
    "x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "win_coeffs = torch.from_numpy(win_coeffs)\n",
    "\n",
    "# Timing pfb_fir_frontend_TINA_FFT\n",
    "xinput = np.random.rand(batchsize,*x.shape)\n",
    "PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "PFB_layer = PFB_layer.float()\n",
    "xinput = torch.from_numpy(xinput).float()\n",
    "output = PFB_layer(xinput)\n",
    "\n",
    "print(output)\n",
    "print(PFB_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to ONNX\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:16.677172\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1381.91tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/pfb_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import vai_q_onnx\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    ")\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_15-32-17.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Specify the path to the quantized ONNZ Model\n",
    "onnx_model_path = \"models/pfb_quantized.onnx\"\n",
    "\n",
    "# Create some random input data for testing\n",
    "input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "cpu_options.enable_profiling = True\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "start = timer()\n",
    "cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "cpu_total = timer() - start\n",
    "\n",
    "cpu_session.end_profiling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-06-17_15-32-18.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and run\n",
    "\n",
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"vaip_config.json\"\n",
    "\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "aie_options.enable_profiling = True\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options=[{'config_file': config_file_path}]\n",
    ")\n",
    "\n",
    "start = timer()\n",
    "ryzen_outputs = aie_session.run(None, {'input': input_data})\n",
    "aie_total = timer() - start\n",
    "\n",
    "aie_session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryzen Results: [array([[[[ 0.015625  , -0.04296875, -0.046875  , ...,  0.015625  ,\n",
      "           0.01953125,  0.046875  ]],\n",
      "\n",
      "        [[ 0.0234375 , -0.01171875,  0.01171875, ...,  0.03515625,\n",
      "           0.015625  , -0.01953125]],\n",
      "\n",
      "        [[-0.00390625, -0.03125   ,  0.015625  , ...,  0.02734375,\n",
      "          -0.0078125 ,  0.00390625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.01171875,  0.03125   , -0.04296875, ...,  0.0234375 ,\n",
      "          -0.015625  ,  0.00390625]],\n",
      "\n",
      "        [[-0.0234375 , -0.015625  ,  0.01171875, ...,  0.015625  ,\n",
      "           0.0234375 , -0.00390625]],\n",
      "\n",
      "        [[-0.01171875,  0.0078125 ,  0.        , ...,  0.0078125 ,\n",
      "           0.0390625 , -0.01171875]]]], dtype=float32)]\n",
      "CPU Results: [array([[[[ 0.015625  , -0.04296875, -0.046875  , ...,  0.015625  ,\n",
      "           0.01953125,  0.046875  ]],\n",
      "\n",
      "        [[ 0.0234375 , -0.01171875,  0.01171875, ...,  0.03515625,\n",
      "           0.015625  , -0.01953125]],\n",
      "\n",
      "        [[-0.00390625, -0.03125   ,  0.015625  , ...,  0.02734375,\n",
      "          -0.0078125 ,  0.00390625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.01171875,  0.03125   , -0.04296875, ...,  0.0234375 ,\n",
      "          -0.015625  ,  0.00390625]],\n",
      "\n",
      "        [[-0.0234375 , -0.015625  ,  0.01171875, ...,  0.015625  ,\n",
      "           0.0234375 , -0.00390625]],\n",
      "\n",
      "        [[-0.01171875,  0.0078125 ,  0.        , ...,  0.0078125 ,\n",
      "           0.0390625 , -0.01171875]]]], dtype=float32)]\n",
      "CPU Total Time: 0.0008623999999999299\n",
      "IPU Total Time: 0.0008954000000001017\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ryzen Results: {ryzen_outputs}\")\n",
    "print(f\"CPU Results: {cpu_results}\")\n",
    "\n",
    "print(f\"CPU Total Time: {cpu_total}\")\n",
    "print(f\"IPU Total Time: {aie_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:21.073098\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3499.00tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:22.783538\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 6993.84tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 1920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:24.751039\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 445.20tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 2560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:27.121158\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 3200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:29.810008\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 6972.25tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 3840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:32.856964\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3500.25tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 4480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:36.174210\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3499.84tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 5120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:39.896848\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3501.51tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 5760] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:43.904285\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 6400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:48.301612\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 7040] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:53.004909\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3501.09tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 7680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:32:58.044084\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3498.17tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 8320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:03.548140\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 8960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:09.268314\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2334.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 9600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:15.500756\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 5278.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 10240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:21.940840\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2333.32tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  1 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 2560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:28.750966\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:33:35.927527 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:33:35.931528 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 5120] type <class 'numpy.float32'> \n",
      "I0617 15:33:35.945528 16056 quant_utils.py:507] Random input name input shape [1, 5120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:33:35.953530 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:33:35.958527 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:33:35.961530 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:33:35.961530 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:33:35.981528 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:33:35.999529 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:33:36.019385 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:33:36.021386 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:33:36.021386 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:35.920529\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 7000.51tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:33:36.037385 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:33:43.342797 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:33:43.348790 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 7680] type <class 'numpy.float32'> \n",
      "I0617 15:33:43.359758 16056 quant_utils.py:507] Random input name input shape [1, 7680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:33:43.359758 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:33:43.359758 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:33:43.372478 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:33:43.372478 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:33:43.389424 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:33:43.405959 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:33:43.432278 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:33:43.434277 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:33:43.434277 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:43.327174\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3197.23tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:33:43.449515 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:33:51.128429 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:33:51.136930 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 10240] type <class 'numpy.float32'> \n",
      "I0617 15:33:51.147287 16056 quant_utils.py:507] Random input name input shape [1, 10240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:33:51.147287 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:33:51.147287 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:33:51.161789 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:33:51.162800 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:33:51.181113 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:33:51.197648 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:33:51.214054 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:33:51.214054 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:33:51.214054 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:51.112778\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:33:51.233716 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:33:59.194928 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:33:59.212530 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 12800] type <class 'numpy.float32'> \n",
      "I0617 15:33:59.222398 16056 quant_utils.py:507] Random input name input shape [1, 12800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:33:59.222398 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:33:59.222398 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:33:59.222398 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:33:59.236989 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:33:59.256129 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:33:59.273365 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:33:59.289457 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:33:59.303770 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:33:59.304778 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:33:59.194928\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3499.84tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:33:59.320906 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:07.698503 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:07.703505 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 15360] type <class 'numpy.float32'> \n",
      "I0617 15:34:07.716505 16056 quant_utils.py:507] Random input name input shape [1, 15360] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:07.718504 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:07.723505 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:07.728504 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:07.728504 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:07.749503 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:07.769503 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:07.793977 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:07.794978 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:07.795979 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:07.691503\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2333.87tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:07.815386 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:16.524907 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:16.533575 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "I0617 15:34:16.542310 16056 quant_utils.py:507] Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:16.542310 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:16.542310 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:16.557326 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:16.559058 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:16.574603 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:16.592320 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:16.607362 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:16.616642 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:16.616642 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:16.509279\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1290.90tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:16.628977 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:25.600196 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:25.604198 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 20480] type <class 'numpy.float32'> \n",
      "I0617 15:34:25.618199 16056 quant_utils.py:507] Random input name input shape [1, 20480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:25.623196 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:25.627197 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:25.631197 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:25.632199 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:25.666199 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:25.693197 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:25.710381 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:25.712382 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:25.712382 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:25.593196\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:25.728767 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:35.125406 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:35.125406 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 23040] type <class 'numpy.float32'> \n",
      "I0617 15:34:35.150050 16056 quant_utils.py:507] Random input name input shape [1, 23040] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:35.161157 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:35.162145 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:35.162145 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:35.162145 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:35.195090 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:35.210813 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:35.228491 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:35.228491 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:35.228491 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:35.109783\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2172.09tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:35.259610 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:45.086982 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:45.092104 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 25600] type <class 'numpy.float32'> \n",
      "I0617 15:34:45.106201 16056 quant_utils.py:507] Random input name input shape [1, 25600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:45.109201 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:45.115202 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:45.121212 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:45.122636 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:45.147642 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:45.167651 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:45.185649 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:45.187652 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:45.188652 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:45.080985\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.75tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:45.206652 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:34:55.271293 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:34:55.284644 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 28160] type <class 'numpy.float32'> \n",
      "I0617 15:34:55.299329 16056 quant_utils.py:507] Random input name input shape [1, 28160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:34:55.300552 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:34:55.300552 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:34:55.313076 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:34:55.314085 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:34:55.338270 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:34:55.358285 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:34:55.374283 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:34:55.376286 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:34:55.377286 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:34:55.271293\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.54tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:34:55.395286 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:35:05.909748 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:35:05.915781 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 30720] type <class 'numpy.float32'> \n",
      "I0617 15:35:05.932645 16056 quant_utils.py:507] Random input name input shape [1, 30720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:35:05.937644 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:35:05.943642 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:35:05.949269 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:35:05.950701 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:35:05.971706 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:35:05.991427 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:35:06.008571 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:35:06.009572 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:35:06.010738 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:35:05.903744\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1400.03tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:35:06.030632 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:35:16.931959 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:35:16.936622 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 33280] type <class 'numpy.float32'> \n",
      "I0617 15:35:16.952963 16056 quant_utils.py:507] Random input name input shape [1, 33280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:35:16.956438 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:35:16.959289 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:35:16.959289 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:35:16.968300 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:35:16.990965 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:35:17.021322 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:35:17.038777 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:35:17.038777 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:35:17.038777 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:35:16.919955\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:35:17.059321 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:35:28.329943 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:35:28.335030 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "I0617 15:35:28.351029 16056 quant_utils.py:507] Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:35:28.354027 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:35:28.360028 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:35:28.363028 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:35:28.364031 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:35:28.396293 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:35:28.420288 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:35:28.439539 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:35:28.441539 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:35:28.442539 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:35:28.322528\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1166.61tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:35:28.465611 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:35:39.774879 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:35:39.780208 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 38400] type <class 'numpy.float32'> \n",
      "I0617 15:35:39.798387 16056 quant_utils.py:507] Random input name input shape [1, 38400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:35:39.801389 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:35:39.806387 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:35:39.811409 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:35:39.812413 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:35:39.834213 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:35:39.858143 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:35:39.875140 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:35:39.877144 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:35:39.878144 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:35:39.769451\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1401.17tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:35:39.897527 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:35:51.677351 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:35:51.697245 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "I0617 15:35:51.709874 16056 quant_utils.py:507] Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:35:51.709874 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:35:51.709874 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:35:51.726189 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:35:51.726189 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:35:51.752531 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:35:51.775329 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:35:51.796135 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:35:51.797792 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:35:51.797792 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:35:51.677351\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 674.59tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:35:51.822829 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  4 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:36:03.833702 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:36:03.853155 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 4480] type <class 'numpy.float32'> \n",
      "I0617 15:36:03.855550 16056 quant_utils.py:507] Random input name input shape [1, 4480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:36:03.869066 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:36:03.869066 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:36:03.869066 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:36:03.869066 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:36:03.902482 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:36:03.920920 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:36:03.935593 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:36:03.938181 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:36:03.938181 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:36:03.833702\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:36:03.954875 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:36:16.223222 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:36:16.236929 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 8960] type <class 'numpy.float32'> \n",
      "I0617 15:36:16.249587 16056 quant_utils.py:507] Random input name input shape [1, 8960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:36:16.251175 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:36:16.251175 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:36:16.251175 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:36:16.251175 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:36:16.281991 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:36:16.300661 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:36:16.316379 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:36:16.318058 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:36:16.318058 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:36:16.223222\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:36:16.334863 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:36:28.952199 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:36:28.957201 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 13440] type <class 'numpy.float32'> \n",
      "I0617 15:36:28.970200 16056 quant_utils.py:507] Random input name input shape [1, 13440] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:36:28.973200 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:36:28.977199 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:36:28.982203 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:36:28.983201 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:36:29.004199 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:36:29.022199 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:36:29.040199 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:36:29.042201 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:36:29.043202 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:36:28.945199\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2334.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:36:29.061199 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:36:41.943020 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:36:41.947021 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "I0617 15:36:41.963020 16056 quant_utils.py:507] Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:36:41.967020 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:36:41.972021 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:36:41.988019 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:36:41.989020 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:36:42.018020 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:36:42.038019 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:36:42.055019 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:36:42.057023 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:36:42.057023 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:36:41.935020\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2334.43tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:36:42.076019 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:36:55.313507 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:36:55.317508 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 22400] type <class 'numpy.float32'> \n",
      "I0617 15:36:55.330509 16056 quant_utils.py:507] Random input name input shape [1, 22400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:36:55.333506 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:36:55.339507 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:36:55.345508 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:36:55.346508 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:36:55.367508 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:36:55.386583 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:36:55.403154 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:36:55.404154 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:36:55.405157 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:36:55.305510\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.34tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:36:55.424156 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:37:09.160342 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:37:09.164341 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 26880] type <class 'numpy.float32'> \n",
      "I0617 15:37:09.179344 16056 quant_utils.py:507] Random input name input shape [1, 26880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:37:09.205857 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:37:09.209856 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:37:09.215995 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:37:09.217017 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:37:09.240024 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:37:09.259926 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:37:09.277961 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:37:09.279961 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:37:09.280962 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:37:09.153341\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.96tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:37:09.296100 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:37:23.361724 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:37:23.365724 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 31360] type <class 'numpy.float32'> \n",
      "I0617 15:37:23.381724 16056 quant_utils.py:507] Random input name input shape [1, 31360] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:37:23.384052 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:37:23.390058 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:37:23.395058 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:37:23.396481 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:37:23.423256 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:37:23.444584 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:37:23.461690 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:37:23.463694 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:37:23.463694 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:37:23.354725\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1400.17tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:37:23.482692 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:37:37.739172 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:37:37.744171 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "I0617 15:37:37.760171 16056 quant_utils.py:507] Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:37:37.764171 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:37:37.770171 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:37:37.774173 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:37:37.774173 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:37:37.800361 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:37:37.823176 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:37:37.840436 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:37:37.842439 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:37:37.842439 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:37:37.733173\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1165.04tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:37:37.865249 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:37:52.599601 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:37:52.603983 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40320] type <class 'numpy.float32'> \n",
      "I0617 15:37:52.618987 16056 quant_utils.py:507] Random input name input shape [1, 40320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:37:52.623208 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:37:52.628208 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:37:52.634206 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:37:52.635207 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:37:52.662207 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:37:52.723413 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:37:52.746972 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:37:52.748975 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:37:52.749976 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:37:52.593599\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1166.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:37:52.774416 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:38:07.616102 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:38:07.620578 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 44800] type <class 'numpy.float32'> \n",
      "I0617 15:38:07.634577 16056 quant_utils.py:507] Random input name input shape [1, 44800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:38:07.639577 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:38:07.645577 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:38:07.649576 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:38:07.650579 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:38:07.681543 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:38:07.701018 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:38:07.727871 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:38:07.730875 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:38:07.730875 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:38:07.609101\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 943.93tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:38:07.762379 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:38:23.176115 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:38:23.180115 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 49280] type <class 'numpy.float32'> \n",
      "I0617 15:38:23.194117 16056 quant_utils.py:507] Random input name input shape [1, 49280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:38:23.197116 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:38:23.203115 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:38:23.209578 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:38:23.210733 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:38:23.240028 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:38:23.262028 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:38:23.281027 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:38:23.283257 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:38:23.284265 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:38:23.169114\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 999.39tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:38:23.303155 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:38:38.750442 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:38:38.754440 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 53760] type <class 'numpy.float32'> \n",
      "I0617 15:38:38.769724 16056 quant_utils.py:507] Random input name input shape [1, 53760] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:38:38.772725 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:38:38.778071 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:38:38.781236 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:38:38.782235 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:38:38.813232 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:38:38.835378 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:38:38.854919 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:38:38.856922 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:38:38.856922 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:38:38.744440\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 875.14tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:38:38.881144 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:38:54.981104 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:38:54.985104 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 58240] type <class 'numpy.float32'> \n",
      "I0617 15:38:55.000331 16056 quant_utils.py:507] Random input name input shape [1, 58240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:38:55.004331 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:38:55.009738 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:38:55.014742 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:38:55.015739 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:38:55.047886 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:38:55.071522 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:38:55.089289 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:38:55.091291 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:38:55.092292 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:38:54.972105\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 982.63tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:38:55.118400 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:39:11.725075 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:39:11.729584 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 62720] type <class 'numpy.float32'> \n",
      "I0617 15:39:11.742583 16056 quant_utils.py:507] Random input name input shape [1, 62720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:39:11.746585 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:39:11.752584 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:39:11.757584 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:39:11.758584 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:39:11.783810 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:39:11.802449 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:39:11.819440 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:39:11.822266 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:39:11.822266 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:39:11.718052\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 849.64tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:39:11.847385 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:39:28.634715 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:39:28.644802 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 67200] type <class 'numpy.float32'> \n",
      "I0617 15:39:28.661332 16056 quant_utils.py:507] Random input name input shape [1, 67200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:39:28.661332 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:39:28.669988 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:39:28.678292 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:39:28.679678 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:39:28.705222 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:39:28.721514 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:39:28.738281 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:39:28.738281 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:39:28.738281 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:39:28.619087\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 387.52tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:39:28.773186 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:39:45.674645 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:39:45.678647 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "I0617 15:39:45.692648 16056 quant_utils.py:507] Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:39:45.696650 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:39:45.724658 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:39:45.729658 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:39:45.731048 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:39:45.752374 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:39:45.771826 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:39:45.788951 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:39:45.791786 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:39:45.791786 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:39:45.666647\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 636.38tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:39:45.821787 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  7 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:40:03.104769 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:40:03.112179 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 6400] type <class 'numpy.float32'> \n",
      "I0617 15:40:03.123968 16056 quant_utils.py:507] Random input name input shape [1, 6400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:40:03.125035 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:40:03.125035 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:40:03.125035 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:40:03.125035 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:40:03.157394 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:40:03.174950 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:40:03.191913 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:40:03.191913 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:40:03.191913 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:40:03.089136\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:40:03.212553 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:40:20.849731 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:40:20.853732 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 12800] type <class 'numpy.float32'> \n",
      "I0617 15:40:20.866730 16056 quant_utils.py:507] Random input name input shape [1, 12800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:40:20.868732 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:40:20.872731 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:40:20.876732 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:40:20.876732 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:40:20.897732 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:40:20.916733 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:40:20.933732 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:40:20.936733 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:40:20.936733 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:40:20.842733\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3497.34tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:40:20.955133 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:40:38.686841 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:40:38.699630 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 19200] type <class 'numpy.float32'> \n",
      "I0617 15:40:38.712197 16056 quant_utils.py:507] Random input name input shape [1, 19200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:40:38.712197 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:40:38.742954 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:40:38.748961 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:40:38.748961 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:40:38.762816 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:40:38.819736 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:40:38.832879 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:40:38.832879 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:40:38.832879 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:40:38.686841\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 681.34tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:40:38.861681 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:40:57.183905 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:40:57.188412 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 25600] type <class 'numpy.float32'> \n",
      "I0617 15:40:57.202413 16056 quant_utils.py:507] Random input name input shape [1, 25600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:40:57.206413 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:40:57.211410 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:40:57.216411 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:40:57.217412 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:40:57.246942 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:40:57.272312 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:40:57.289309 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:40:57.291312 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:40:57.292312 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:40:57.178028\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1400.30tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:40:57.314314 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:41:16.169098 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:41:16.174099 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 32000] type <class 'numpy.float32'> \n",
      "I0617 15:41:16.190195 16056 quant_utils.py:507] Random input name input shape [1, 32000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:41:16.196197 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:41:16.201505 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:41:16.207862 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:41:16.208867 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:41:16.234153 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:41:16.258045 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:41:16.277046 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:41:16.279045 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:41:16.279045 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:41:16.163099\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1317.42tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:41:16.305358 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:41:34.817832 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:41:34.825445 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 38400] type <class 'numpy.float32'> \n",
      "I0617 15:41:34.838706 16056 quant_utils.py:507] Random input name input shape [1, 38400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:41:34.842748 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:41:34.845975 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:41:34.854492 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:41:34.855492 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:41:34.874722 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:41:34.905003 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:41:34.921659 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:41:34.923446 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:41:34.923950 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:41:34.802303\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2221.56tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:41:34.939534 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:41:54.208203 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:41:54.216814 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 44800] type <class 'numpy.float32'> \n",
      "I0617 15:41:54.229884 16056 quant_utils.py:507] Random input name input shape [1, 44800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:41:54.233329 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:41:54.236050 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:41:54.244714 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:41:54.244714 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:41:54.274121 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:41:54.293329 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:41:54.309259 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:41:54.309259 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:41:54.309259 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:41:54.199159\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 391.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:41:54.346150 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:42:14.133794 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:42:14.138644 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 51200] type <class 'numpy.float32'> \n",
      "I0617 15:42:14.157151 16056 quant_utils.py:507] Random input name input shape [1, 51200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:42:14.162438 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:42:14.167632 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:42:14.173815 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:42:14.174814 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:42:14.200637 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:42:14.222517 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:42:14.241549 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:42:14.244182 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:42:14.244182 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:42:14.122723\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 777.83tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:42:14.277605 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:42:34.014327 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:42:34.018815 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 57600] type <class 'numpy.float32'> \n",
      "I0617 15:42:34.034934 16056 quant_utils.py:507] Random input name input shape [1, 57600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:42:34.037751 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:42:34.055717 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:42:34.059718 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:42:34.060946 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:42:34.089020 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:42:34.113019 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:42:34.135381 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:42:34.138779 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:42:34.138779 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:42:34.005145\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 925.75tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:42:34.168886 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:42:54.433671 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:42:54.438670 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 64000] type <class 'numpy.float32'> \n",
      "I0617 15:42:54.454927 16056 quant_utils.py:507] Random input name input shape [1, 64000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:42:54.458945 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:42:54.465729 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:42:54.471571 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:42:54.474017 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:42:54.525755 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:42:54.546998 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:42:54.568094 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:42:54.571443 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:42:54.572442 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:42:54.395669\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 699.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:42:54.602735 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:43:15.109729 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:43:15.115869 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 70400] type <class 'numpy.float32'> \n",
      "I0617 15:43:15.131122 16056 quant_utils.py:507] Random input name input shape [1, 70400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:43:15.133061 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:43:15.133061 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:43:15.146009 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:43:15.146009 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:43:15.164437 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:43:15.179382 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:43:15.208591 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:43:15.210600 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:43:15.210600 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:43:15.093047\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:43:15.228239 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:43:36.119195 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:43:36.124195 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 76800] type <class 'numpy.float32'> \n",
      "I0617 15:43:36.141340 16056 quant_utils.py:507] Random input name input shape [1, 76800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:43:36.144341 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:43:36.151341 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:43:36.173442 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:43:36.174442 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:43:36.203629 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:43:36.223628 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:43:36.240627 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:43:36.242742 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:43:36.243749 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:43:36.110195\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 699.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:43:36.274785 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:43:57.610770 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:43:57.628865 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 83200] type <class 'numpy.float32'> \n",
      "I0617 15:43:57.642755 16056 quant_utils.py:507] Random input name input shape [1, 83200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:43:57.645267 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:43:57.645267 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:43:57.654489 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:43:57.655497 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:43:57.678541 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:43:57.698790 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:43:57.720222 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:43:57.722231 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:43:57.723228 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:43:57.610770\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 590.51tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:43:57.754606 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:44:19.206791 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:44:19.221851 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 89600] type <class 'numpy.float32'> \n",
      "I0617 15:44:19.229850 16056 quant_utils.py:507] Random input name input shape [1, 89600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:44:19.238003 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:44:19.246097 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:44:19.250998 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:44:19.250998 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:44:19.277410 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:44:19.302046 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:44:19.320487 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:44:19.322489 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:44:19.323488 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:44:19.206791\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 538.48tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:44:19.357514 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:44:41.279964 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:44:41.283966 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 96000] type <class 'numpy.float32'> \n",
      "I0617 15:44:41.296967 16056 quant_utils.py:507] Random input name input shape [1, 96000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:44:41.299967 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:44:41.302668 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:44:41.302668 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:44:41.302668 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:44:41.335799 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:44:41.353648 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:44:41.369083 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:44:41.382091 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:44:41.383116 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:44:41.273965\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2234.92tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:44:41.403049 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:45:03.653327 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:45:03.658328 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 102400] type <class 'numpy.float32'> \n",
      "I0617 15:45:03.673121 16056 quant_utils.py:507] Random input name input shape [1, 102400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:45:03.679674 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:45:03.686487 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:45:03.692586 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:45:03.693585 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:45:03.718567 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:45:03.743489 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:45:03.762506 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:45:03.765506 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:45:03.765506 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:45:03.646327\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 362.57tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:45:03.805227 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  10 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:45:26.365893 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:45:26.369894 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 8320] type <class 'numpy.float32'> \n",
      "I0617 15:45:26.382893 16056 quant_utils.py:507] Random input name input shape [1, 8320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:45:26.387894 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:45:26.392892 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:45:26.395894 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:45:26.395894 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:45:26.418892 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:45:26.438892 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:45:26.455893 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:45:26.456893 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:45:26.457894 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:45:26.359382\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3501.09tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:45:26.488856 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:45:49.311698 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:45:49.322668 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 16640] type <class 'numpy.float32'> \n",
      "I0617 15:45:49.325608 16056 quant_utils.py:507] Random input name input shape [1, 16640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:45:49.338546 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:45:49.338546 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:45:49.338546 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:45:49.338546 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:45:49.355286 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:45:49.371977 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:45:49.405364 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:45:49.405364 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:45:49.405364 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:45:49.296073\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:45:49.422006 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:46:12.648062 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:46:12.664495 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 24960] type <class 'numpy.float32'> \n",
      "I0617 15:46:12.677494 16056 quant_utils.py:507] Random input name input shape [1, 24960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:46:12.680496 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:46:12.683491 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:46:12.691481 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:46:12.692672 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:46:12.713605 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:46:12.736852 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:46:12.749619 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:46:12.749619 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:46:12.749619 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:46:12.648062\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 451.57tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:46:12.780166 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:46:36.090402 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:46:36.096414 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 33280] type <class 'numpy.float32'> \n",
      "I0617 15:46:36.114463 16056 quant_utils.py:507] Random input name input shape [1, 33280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:46:36.118057 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:46:36.125232 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:46:36.130231 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:46:36.132229 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:46:36.161643 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:46:36.182974 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:46:36.202686 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:46:36.204621 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:46:36.204621 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:46:36.082959\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1528.85tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:46:36.231971 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:47:00.178900 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:47:00.183902 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 41600] type <class 'numpy.float32'> \n",
      "I0617 15:47:00.199108 16056 quant_utils.py:507] Random input name input shape [1, 41600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:47:00.203106 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:47:00.209107 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:47:00.214105 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:47:00.215105 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:47:00.242910 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:47:00.265035 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:47:00.284029 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:47:00.285032 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:47:00.286032 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:47:00.172899\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1165.82tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:47:00.313958 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:47:24.456060 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:47:24.460063 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 49920] type <class 'numpy.float32'> \n",
      "I0617 15:47:24.473061 16056 quant_utils.py:507] Random input name input shape [1, 49920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:47:24.476977 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:47:24.483299 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:47:24.489053 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:47:24.490160 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:47:24.518719 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:47:24.540851 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:47:24.559772 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:47:24.561215 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:47:24.562222 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:47:24.448058\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 888.60tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:47:24.591471 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:47:49.184257 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:47:49.189256 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 58240] type <class 'numpy.float32'> \n",
      "I0617 15:47:49.204695 16056 quant_utils.py:507] Random input name input shape [1, 58240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:47:49.209700 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:47:49.215693 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:47:49.222694 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:47:49.223694 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:47:49.251211 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:47:49.272727 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:47:49.291726 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:47:49.294731 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:47:49.295727 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:47:49.177255\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 996.07tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:47:49.334945 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:48:14.055800 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:48:14.060805 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 66560] type <class 'numpy.float32'> \n",
      "I0617 15:48:14.075075 16056 quant_utils.py:507] Random input name input shape [1, 66560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:48:14.079075 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:48:14.085527 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:48:14.090802 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:48:14.093162 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:48:14.131160 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:48:14.151788 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:48:14.170653 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:48:14.173935 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:48:14.173935 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:48:14.031800\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 699.55tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:48:14.209419 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:48:39.122583 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:48:39.126582 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 74880] type <class 'numpy.float32'> \n",
      "I0617 15:48:39.140613 16056 quant_utils.py:507] Random input name input shape [1, 74880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:48:39.143613 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:48:39.148612 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:48:39.152612 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:48:39.153613 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:48:39.185611 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:48:39.205614 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:48:39.223938 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:48:39.226413 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:48:39.226413 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:48:39.115583\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 635.97tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:48:39.260791 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:49:04.727930 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:49:04.732333 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 83200] type <class 'numpy.float32'> \n",
      "I0617 15:49:04.748332 16056 quant_utils.py:507] Random input name input shape [1, 83200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:49:04.751329 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:49:04.758333 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:49:04.765789 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:49:04.766790 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:49:04.794539 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:49:04.818647 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:49:04.839196 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:49:04.842198 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:49:04.843197 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:49:04.718683\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 636.34tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:49:04.881970 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:49:30.751830 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:49:30.755831 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 91520] type <class 'numpy.float32'> \n",
      "I0617 15:49:30.770199 16056 quant_utils.py:507] Random input name input shape [1, 91520] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:49:30.772202 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:49:30.778936 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:49:30.784932 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:49:30.785933 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:49:30.813931 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:49:30.832933 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:49:30.851526 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:49:30.853529 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:49:30.854961 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:49:30.744830\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 578.15tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:49:30.890231 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:49:56.793237 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:49:56.798237 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 99840] type <class 'numpy.float32'> \n",
      "I0617 15:49:56.811236 16056 quant_utils.py:507] Random input name input shape [1, 99840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:49:56.814322 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:49:56.819651 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:49:56.826648 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:49:56.827647 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:49:56.856729 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:49:56.877086 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:49:56.896098 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:49:56.898096 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:49:56.899099 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:49:56.787236\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 567.78tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:49:56.933740 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:50:23.453732 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:50:23.458733 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 108160] type <class 'numpy.float32'> \n",
      "I0617 15:50:23.471733 16056 quant_utils.py:507] Random input name input shape [1, 108160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:50:23.473733 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:50:23.480086 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:50:23.487081 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:50:23.488082 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:50:23.519473 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:50:23.541474 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:50:23.559473 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:50:23.561477 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:50:23.561477 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:50:23.446732\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 535.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:50:23.601019 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:50:50.463828 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:50:50.482256 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 116480] type <class 'numpy.float32'> \n",
      "I0617 15:50:50.495273 16056 quant_utils.py:507] Random input name input shape [1, 116480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:50:50.496809 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:50:50.496809 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:50:50.508999 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:50:50.510001 16056 quantize.py:213] Loading model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:50:50.463828\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:50:50.676060 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:50:50.699642 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:50:50.793768 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:50:50.795753 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:50:50.796799 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 437.75tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:50:50.840828 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:51:17.717961 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:51:17.730596 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 124800] type <class 'numpy.float32'> \n",
      "I0617 15:51:17.742297 16056 quant_utils.py:507] Random input name input shape [1, 124800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:51:17.742297 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:51:17.756501 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:51:17.759101 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:51:17.759101 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:51:17.790274 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:51:17.805462 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:51:17.822133 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:51:17.822133 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:51:17.822133 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:51:17.717961\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 473.31tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:51:17.871325 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:51:45.326821 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:51:45.340458 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 133120] type <class 'numpy.float32'> \n",
      "I0617 15:51:45.355334 16056 quant_utils.py:507] Random input name input shape [1, 133120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:51:45.358301 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:51:45.378108 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:51:45.384149 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:51:45.384149 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:51:45.407418 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:51:45.424434 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:51:45.443532 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:51:45.450476 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:51:45.451472 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:51:45.326821\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 321.67tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:51:45.504778 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  13 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:52:13.385780 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:52:13.389779 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 10240] type <class 'numpy.float32'> \n",
      "I0617 15:52:13.405778 16056 quant_utils.py:507] Random input name input shape [1, 10240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:52:13.408781 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:52:13.413780 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:52:13.418782 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:52:13.419781 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:52:13.445781 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:52:13.466036 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:52:13.484524 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:52:13.486524 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:52:13.486524 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:52:13.377779\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3499.00tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:52:13.517256 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:52:41.276576 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:52:41.281039 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 20480] type <class 'numpy.float32'> \n",
      "I0617 15:52:41.295039 16056 quant_utils.py:507] Random input name input shape [1, 20480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:52:41.296120 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:52:41.296120 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:52:41.296120 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:52:41.305773 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:52:41.328814 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:52:41.346469 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:52:41.374325 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:52:41.376828 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:52:41.376828 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:52:41.268578\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:52:41.411616 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:53:09.667504 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:53:09.671506 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 30720] type <class 'numpy.float32'> \n",
      "I0617 15:53:09.687046 16056 quant_utils.py:507] Random input name input shape [1, 30720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:53:09.691284 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:53:09.697830 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:53:09.700217 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:53:09.701717 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:53:09.725032 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:53:09.744934 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:53:09.762028 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:53:09.763027 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:53:09.764029 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:53:09.660504\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.86tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:53:09.794668 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:53:38.287083 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:53:38.291422 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "I0617 15:53:38.305145 16056 quant_utils.py:507] Random input name input shape [1, 40960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:53:38.308342 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:53:38.311639 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:53:38.317987 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:53:38.318996 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:53:38.353281 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:53:38.371045 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:53:38.388020 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:53:38.388020 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:53:38.388020 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:53:38.271425\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 456.46tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:53:38.426511 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:54:07.312082 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:54:07.323964 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 51200] type <class 'numpy.float32'> \n",
      "I0617 15:54:07.338857 16056 quant_utils.py:507] Random input name input shape [1, 51200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:54:07.339669 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:54:07.348581 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:54:07.353208 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:54:07.353208 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:54:07.385969 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:54:07.401883 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:54:07.418309 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:54:07.427520 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:54:07.427520 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:54:07.296139\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 658.55tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:54:07.453282 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:54:36.879737 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:54:36.885737 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 61440] type <class 'numpy.float32'> \n",
      "I0617 15:54:36.899736 16056 quant_utils.py:507] Random input name input shape [1, 61440] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:54:36.903736 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:54:36.909742 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:54:36.913233 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:54:36.914234 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:54:36.942775 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:54:36.962215 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:54:36.980931 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:54:36.982441 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:54:36.983441 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:54:36.873736\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 777.85tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:54:37.018826 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:55:06.785346 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:55:06.789348 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "I0617 15:55:06.804361 16056 quant_utils.py:507] Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:55:06.809345 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:55:06.816350 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:55:06.822483 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:55:06.823802 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:55:06.860666 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:55:06.887082 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:55:06.907198 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:55:06.911201 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:55:06.911201 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:55:06.778345\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 699.90tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:55:06.950789 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:55:37.293740 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:55:37.310563 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 81920] type <class 'numpy.float32'> \n",
      "I0617 15:55:37.324562 16056 quant_utils.py:507] Random input name input shape [1, 81920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:55:37.328565 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:55:37.334565 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:55:37.339109 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:55:37.341119 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:55:37.372771 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:55:37.393935 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:55:37.410899 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:55:37.412906 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:55:37.413905 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:55:37.293740\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 559.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:55:37.454129 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:56:07.919195 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:56:07.928860 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 92160] type <class 'numpy.float32'> \n",
      "I0617 15:56:07.933127 16056 quant_utils.py:507] Random input name input shape [1, 92160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:56:07.944848 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:56:07.947258 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:56:07.947258 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:56:07.947258 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:56:07.993966 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:56:08.012820 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:56:08.030716 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:56:08.030716 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:56:08.030716 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:56:07.904156\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 328.38tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:56:08.078460 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:56:39.019796 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:56:39.024798 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 102400] type <class 'numpy.float32'> \n",
      "I0617 15:56:39.038799 16056 quant_utils.py:507] Random input name input shape [1, 102400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:56:39.043130 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:56:39.049194 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:56:39.054986 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:56:39.056147 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:56:39.086308 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:56:39.109315 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:56:39.128313 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:56:39.131316 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:56:39.131316 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:56:39.012796\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 538.30tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:56:39.175829 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:57:10.672791 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:57:10.691210 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 112640] type <class 'numpy.float32'> \n",
      "I0617 15:57:10.695422 16056 quant_utils.py:507] Random input name input shape [1, 112640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:57:10.710423 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:57:10.716429 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:57:10.722968 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:57:10.725934 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:57:10.754042 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:57:10.779431 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:57:10.796135 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:57:10.796135 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:57:10.796135 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:57:10.672792\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 278.39tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:57:10.848872 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:57:42.567210 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:57:42.571212 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 122880] type <class 'numpy.float32'> \n",
      "I0617 15:57:42.587210 16056 quant_utils.py:507] Random input name input shape [1, 122880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:57:42.589212 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:57:42.595212 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:57:42.599214 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:57:42.601157 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:57:42.632162 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:57:42.654099 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:57:42.673097 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:57:42.675101 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:57:42.676100 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:57:42.558691\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 437.62tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:57:42.720122 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:58:14.457201 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:58:14.462202 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 133120] type <class 'numpy.float32'> \n",
      "I0617 15:58:14.496207 16056 quant_utils.py:507] Random input name input shape [1, 133120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:58:14.499207 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:58:14.505208 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:58:14.509559 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:58:14.510623 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:58:14.540505 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:58:14.563739 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:58:14.586295 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:58:14.589299 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:58:14.590297 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:58:14.448201\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 411.77tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:58:14.647200 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:58:46.895316 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:58:46.899315 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 143360] type <class 'numpy.float32'> \n",
      "I0617 15:58:46.914722 16056 quant_utils.py:507] Random input name input shape [1, 143360] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:58:46.918721 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:58:46.924979 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:58:46.930982 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:58:46.931980 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:58:46.963993 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:58:46.985994 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:58:47.006262 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:58:47.009264 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:58:47.010264 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:58:46.888314\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 388.93tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:58:47.068286 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:59:19.803065 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:59:19.808202 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 153600] type <class 'numpy.float32'> \n",
      "I0617 15:59:19.824558 16056 quant_utils.py:507] Random input name input shape [1, 153600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:59:19.826097 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:59:19.834247 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:59:19.838636 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:59:19.839632 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:59:19.864674 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:59:19.882817 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:59:19.899017 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:59:19.899017 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:59:19.899017 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:59:19.796035\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 187.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:59:19.962771 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 15:59:52.568876 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 15:59:52.574701 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 163840] type <class 'numpy.float32'> \n",
      "I0617 15:59:52.603394 16056 quant_utils.py:507] Random input name input shape [1, 163840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 15:59:52.606835 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 15:59:52.612439 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 15:59:52.620800 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 15:59:52.622164 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 15:59:52.649616 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 15:59:52.672783 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 15:59:52.695872 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 15:59:52.700468 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 15:59:52.700973 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 15:59:52.561973\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 181.22tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 15:59:52.779729 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  16 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:00:26.040552 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:00:26.045618 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 12160] type <class 'numpy.float32'> \n",
      "I0617 16:00:26.061615 16056 quant_utils.py:507] Random input name input shape [1, 12160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:00:26.064617 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:00:26.069617 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:00:26.072617 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:00:26.073617 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:00:26.166146 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:00:26.188607 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:00:26.211516 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:00:26.212516 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:00:26.213519 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:00:26.033554\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2333.13tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:00:26.245517 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:00:59.593070 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:00:59.610440 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 24320] type <class 'numpy.float32'> \n",
      "I0617 16:00:59.628121 16056 quant_utils.py:507] Random input name input shape [1, 24320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:00:59.630121 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:00:59.635035 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:00:59.637463 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:00:59.641503 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:00:59.670210 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:00:59.688568 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:00:59.767848 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:00:59.780249 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:00:59.593070\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:00:59.781279 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 5773.87tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:00:59.818912 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:01:33.400876 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:01:33.405584 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 36480] type <class 'numpy.float32'> \n",
      "I0617 16:01:33.418581 16056 quant_utils.py:507] Random input name input shape [1, 36480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:01:33.420583 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:01:33.440583 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:01:33.443695 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:01:33.445144 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:01:33.474149 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:01:33.493797 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:01:33.513298 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:01:33.514744 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:01:33.515753 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:01:33.392868\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1275.53tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:01:33.552390 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:02:07.276432 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:02:07.281210 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 48640] type <class 'numpy.float32'> \n",
      "I0617 16:02:07.296210 16056 quant_utils.py:507] Random input name input shape [1, 48640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:02:07.300216 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:02:07.305476 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:02:07.311476 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:02:07.312715 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:02:07.343528 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:02:07.367107 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:02:07.390860 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:02:07.394543 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:02:07.395549 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:02:07.268229\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1027.66tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:02:07.437776 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:02:41.973428 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:02:41.983058 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 60800] type <class 'numpy.float32'> \n",
      "I0617 16:02:41.999088 16056 quant_utils.py:507] Random input name input shape [1, 60800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:02:42.001291 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:02:42.009935 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:02:42.014941 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:02:42.016940 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:02:42.046210 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:02:42.063196 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:02:42.079968 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:02:42.079968 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:02:42.079968 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:02:41.966024\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 380.49tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:02:42.128655 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:03:16.814306 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:03:16.819307 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 72960] type <class 'numpy.float32'> \n",
      "I0617 16:03:16.831308 16056 quant_utils.py:507] Random input name input shape [1, 72960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:03:16.834382 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:03:16.840700 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:03:16.847338 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:03:16.848581 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:03:16.880560 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:03:16.902111 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:03:16.922829 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:03:16.924831 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:03:16.925832 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:03:16.807310\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 636.40tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:03:16.969177 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:03:52.075102 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:03:52.079489 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 85120] type <class 'numpy.float32'> \n",
      "I0617 16:03:52.096594 16056 quant_utils.py:507] Random input name input shape [1, 85120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:03:52.099872 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:03:52.104340 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:03:52.110149 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:03:52.111152 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:03:52.175612 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:03:52.197613 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:03:52.013498\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0617 16:03:52.220251 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:03:52.224250 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:03:52.225251 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 636.40tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:03:52.277699 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:04:27.592262 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:04:27.596263 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 97280] type <class 'numpy.float32'> \n",
      "I0617 16:04:27.611262 16056 quant_utils.py:507] Random input name input shape [1, 97280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:04:27.615263 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:04:27.622301 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:04:27.629297 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:04:27.630300 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:04:27.727083 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:04:27.573262\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:04:27.785397 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:04:27.810158 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:04:27.814566 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:04:27.815566 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 538.20tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:04:27.864387 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:05:03.452369 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:05:03.461302 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 109440] type <class 'numpy.float32'> \n",
      "I0617 16:05:03.474803 16056 quant_utils.py:507] Random input name input shape [1, 109440] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:05:03.477176 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:05:03.480836 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:05:03.480836 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:05:03.480836 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:05:03.526502 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:05:03.546310 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:05:03.563247 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:05:03.563247 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:05:03.563247 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:05:03.436765\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 289.37tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:05:03.618774 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:05:39.678712 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:05:39.691338 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 121600] type <class 'numpy.float32'> \n",
      "I0617 16:05:39.706756 16056 quant_utils.py:507] Random input name input shape [1, 121600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:05:39.706756 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:05:39.726504 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:05:39.732624 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:05:39.733632 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:05:39.759499 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:05:39.785387 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:05:39.828818 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:05:39.830239 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:05:39.831638 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:05:39.671864\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 435.07tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:05:39.902082 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:06:16.205254 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:06:16.209334 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 133760] type <class 'numpy.float32'> \n",
      "I0617 16:06:16.224335 16056 quant_utils.py:507] Random input name input shape [1, 133760] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:06:16.228805 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:06:16.235473 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:06:16.240474 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:06:16.241475 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:06:16.268485 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:06:16.290270 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:06:16.310322 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:06:16.312329 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:06:16.313326 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:06:16.198253\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 437.47tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:06:16.365324 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:06:53.504620 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:06:53.509002 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 145920] type <class 'numpy.float32'> \n",
      "I0617 16:06:53.523998 16056 quant_utils.py:507] Random input name input shape [1, 145920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:06:53.527999 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:06:53.551996 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:06:53.557520 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:06:53.558687 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:06:53.584921 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:06:53.605922 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:06:53.646166 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:06:53.650172 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:06:53.651170 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:06:53.493216\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 233.30tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:06:53.732656 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:07:30.803669 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:07:30.808668 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 158080] type <class 'numpy.float32'> \n",
      "I0617 16:07:30.822669 16056 quant_utils.py:507] Random input name input shape [1, 158080] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:07:30.825668 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:07:30.832672 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:07:30.839670 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:07:30.840669 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:07:30.870943 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:07:30.893289 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:07:30.913721 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:07:30.916721 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:07:30.917721 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:07:30.796668\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 239.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:07:30.982370 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:08:08.090879 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:08:08.095485 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 170240] type <class 'numpy.float32'> \n",
      "I0617 16:08:08.110483 16056 quant_utils.py:507] Random input name input shape [1, 170240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:08:08.115501 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:08:08.123705 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:08:08.130054 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:08:08.132062 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:08:08.162743 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:08:08.185226 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:08:08.209239 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:08:08.211241 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:08:08.212244 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:08:08.082064\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 208.84tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:08:08.286277 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:08:46.192816 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:08:46.198107 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 182400] type <class 'numpy.float32'> \n",
      "I0617 16:08:46.214507 16056 quant_utils.py:507] Random input name input shape [1, 182400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:08:46.220504 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:08:46.226732 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:08:46.232727 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:08:46.233729 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:08:46.261725 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:08:46.283727 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:08:46.303980 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:08:46.306459 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:08:46.307456 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:08:46.185815\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 217.36tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:08:46.378331 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:09:24.473954 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:09:24.477587 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 194560] type <class 'numpy.float32'> \n",
      "I0617 16:09:24.497586 16056 quant_utils.py:507] Random input name input shape [1, 194560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:09:24.501089 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:09:24.519114 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:09:24.523082 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:09:24.524590 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:09:24.563706 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:09:24.587370 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:09:24.609507 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:09:24.613688 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:09:24.613688 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:09:24.459874\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 186.89tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:09:24.701435 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  19 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:10:03.538777 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:10:03.543161 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 14080] type <class 'numpy.float32'> \n",
      "I0617 16:10:03.556160 16056 quant_utils.py:507] Random input name input shape [1, 14080] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:10:03.559161 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:10:03.564160 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:10:03.570159 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:10:03.571160 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:10:03.597161 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:10:03.617532 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:10:03.638248 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:10:03.640249 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:10:03.640249 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:10:03.530777\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2333.87tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:10:03.686292 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:10:41.376632 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:10:41.381633 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 28160] type <class 'numpy.float32'> \n",
      "I0617 16:10:41.396013 16056 quant_utils.py:507] Random input name input shape [1, 28160] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:10:41.399218 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:10:41.404219 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:10:41.421219 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:10:41.422218 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:10:41.450218 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:10:41.471220 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:10:41.491954 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:10:41.492955 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:10:41.493955 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:10:41.368633\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:10:41.535390 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:11:20.294827 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:11:20.298828 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 42240] type <class 'numpy.float32'> \n",
      "I0617 16:11:20.312826 16056 quant_utils.py:507] Random input name input shape [1, 42240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:11:20.314830 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:11:20.320829 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:11:20.327101 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:11:20.328101 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:11:20.356761 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:11:20.379294 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:11:20.399292 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:11:20.401627 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:11:20.402132 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:11:20.286829\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1165.69tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:11:20.447618 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:11:59.241917 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:11:59.246402 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 56320] type <class 'numpy.float32'> \n",
      "I0617 16:11:59.261806 16056 quant_utils.py:507] Random input name input shape [1, 56320] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:11:59.272803 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:11:59.277806 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:11:59.281802 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:11:59.282804 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:11:59.311048 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:11:59.332854 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:11:59.353760 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:11:59.355441 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:11:59.355441 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:11:59.234920\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:11:59.401676 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:12:38.491145 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:12:38.496145 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 70400] type <class 'numpy.float32'> \n",
      "I0617 16:12:38.513570 16056 quant_utils.py:507] Random input name input shape [1, 70400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:12:38.516576 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:12:38.521020 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:12:38.527118 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:12:38.528396 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:12:38.560944 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:12:38.583420 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:12:38.604843 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:12:38.607314 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:12:38.608314 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:12:38.479143\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 401.00tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:12:38.676653 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:13:18.272650 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:13:18.277650 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 84480] type <class 'numpy.float32'> \n",
      "I0617 16:13:18.295649 16056 quant_utils.py:507] Random input name input shape [1, 84480] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:13:18.298651 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:13:18.304131 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:13:18.308426 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:13:18.309424 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:13:18.339913 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:13:18.361144 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:13:18.381706 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:13:18.384709 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:13:18.384709 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:13:18.265650\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 636.13tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:13:18.435270 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:13:58.922906 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:13:58.926910 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 98560] type <class 'numpy.float32'> \n",
      "I0617 16:13:58.940921 16056 quant_utils.py:507] Random input name input shape [1, 98560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:13:58.943919 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:13:58.948920 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:13:58.955919 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:13:58.956919 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:13:58.989837 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:13:59.008984 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:13:59.029550 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:13:59.031554 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:13:59.032552 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:13:58.915906\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 569.40tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:13:59.082323 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:14:39.525315 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:14:39.529636 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 112640] type <class 'numpy.float32'> \n",
      "I0617 16:14:39.544636 16056 quant_utils.py:507] Random input name input shape [1, 112640] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:14:39.548637 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:14:39.553639 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:14:39.560636 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:14:39.562636 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:14:39.591636 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:14:39.618642 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:14:39.643271 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:14:39.645274 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:14:39.646274 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:14:39.516316\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 500.04tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:14:39.702597 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:15:20.162642 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:15:20.172221 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 126720] type <class 'numpy.float32'> \n",
      "I0617 16:15:20.185689 16056 quant_utils.py:507] Random input name input shape [1, 126720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:15:20.201218 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:15:20.203219 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:15:20.213166 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:15:20.214167 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:15:20.242471 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:15:20.278186 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:15:20.299161 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:15:20.302063 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:15:20.302063 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:15:20.156938\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 342.78tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:15:20.363415 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:16:01.640727 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:16:01.650274 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 140800] type <class 'numpy.float32'> \n",
      "I0617 16:16:01.664296 16056 quant_utils.py:507] Random input name input shape [1, 140800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:16:01.664296 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:16:01.677452 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:16:01.739739 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:16:01.739739 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:16:01.774779 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:16:01.793456 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:16:01.625104\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0617 16:16:01.813779 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:16:01.823028 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:16:01.824494 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 382.25tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:16:01.884890 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:16:43.348769 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:16:43.352745 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 154880] type <class 'numpy.float32'> \n",
      "I0617 16:16:43.371476 16056 quant_utils.py:507] Random input name input shape [1, 154880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:16:43.374477 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:16:43.379974 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:16:43.382973 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:16:43.383975 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:16:43.432595 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:16:43.452357 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:16:43.476419 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:16:43.480684 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:16:43.481681 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:16:43.316747\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 225.77tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:16:43.556701 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:17:25.768233 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:17:25.774231 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 168960] type <class 'numpy.float32'> \n",
      "I0617 16:17:25.789231 16056 quant_utils.py:507] Random input name input shape [1, 168960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:17:25.793232 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:17:25.800232 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:17:25.806230 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:17:25.807233 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:17:25.840756 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:17:25.862128 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:17:25.886966 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:17:25.889968 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:17:25.890967 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:17:25.759233\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 177.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:17:25.984969 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:18:08.198405 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:18:08.217038 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 183040] type <class 'numpy.float32'> \n",
      "I0617 16:18:08.229337 16056 quant_utils.py:507] Random input name input shape [1, 183040] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:18:08.229337 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:18:08.242969 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:18:08.247631 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:18:08.248632 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:18:08.274173 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:18:08.293670 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:18:08.310913 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:18:08.323319 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:18:08.324327 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:18:08.198405\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 227.08tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:18:08.390099 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:18:51.289707 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:18:51.295708 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 197120] type <class 'numpy.float32'> \n",
      "I0617 16:18:51.312707 16056 quant_utils.py:507] Random input name input shape [1, 197120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:18:51.316708 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:18:51.323711 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:18:51.330797 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:18:51.332192 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:18:51.360088 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:18:51.381748 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:18:51.406422 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:18:51.410426 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:18:51.411422 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:18:51.277711\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 187.75tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:18:51.496955 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:19:34.455125 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:19:34.460125 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 211200] type <class 'numpy.float32'> \n",
      "I0617 16:19:34.474123 16056 quant_utils.py:507] Random input name input shape [1, 211200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:19:34.477126 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:19:34.484614 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:19:34.490616 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:19:34.491615 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:19:34.523612 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:19:34.543700 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:19:34.564782 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:19:34.568285 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:19:34.568285 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:19:34.447126\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 194.42tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:19:34.648343 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:20:17.440034 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:20:17.450332 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 225280] type <class 'numpy.float32'> \n",
      "I0617 16:20:17.464903 16056 quant_utils.py:507] Random input name input shape [1, 225280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:20:17.466241 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:20:17.466241 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:20:17.481619 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:20:17.482619 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:20:17.513924 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:20:17.534019 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:20:17.550586 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:20:17.550586 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:20:17.550586 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:20:17.424400\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 173.00tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:20:17.642517 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  22 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:21:01.089157 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:21:01.099636 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 16000] type <class 'numpy.float32'> \n",
      "I0617 16:21:01.113198 16056 quant_utils.py:507] Random input name input shape [1, 16000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:21:01.113198 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:21:01.113198 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:21:01.124807 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:21:01.125806 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:21:01.159285 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:21:01.178300 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:21:01.194348 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:21:01.194348 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:21:01.194348 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:21:01.073527\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 2329.06tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:21:01.264321 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:21:44.619747 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:21:44.624109 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 32000] type <class 'numpy.float32'> \n",
      "I0617 16:21:44.638073 16056 quant_utils.py:507] Random input name input shape [1, 32000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:21:44.640075 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:21:44.646073 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:21:44.650710 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:21:44.651738 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:21:44.691727 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:21:44.712877 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:21:44.733877 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:21:44.734877 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:21:44.736184 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:21:44.603747\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1404.12tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:21:44.784213 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:22:28.793050 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:22:28.797223 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 48000] type <class 'numpy.float32'> \n",
      "I0617 16:22:28.815602 16056 quant_utils.py:507] Random input name input shape [1, 48000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:22:28.817648 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:22:28.820937 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:22:28.820937 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:22:28.832250 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:22:28.860292 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:22:28.883352 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:22:28.906611 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:22:28.907962 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:22:28.907962 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:22:28.784304\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 857.23tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:22:28.964266 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:23:13.047876 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:23:13.052293 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 64000] type <class 'numpy.float32'> \n",
      "I0617 16:23:13.068346 16056 quant_utils.py:507] Random input name input shape [1, 64000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:23:13.071574 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:23:13.073943 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:23:13.082408 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:23:13.083883 16056 quantize.py:213] Loading model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:23:13.037722\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:23:13.291108 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:23:13.327076 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:23:13.350600 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:23:13.353440 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:23:13.353943 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 753.73tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:23:13.409128 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:23:57.855965 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:23:57.868498 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 80000] type <class 'numpy.float32'> \n",
      "I0617 16:23:57.882451 16056 quant_utils.py:507] Random input name input shape [1, 80000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:23:57.886925 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:23:57.893207 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:23:57.898213 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:23:57.899214 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:23:57.930617 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:23:57.951544 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:23:57.965426 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:23:57.977597 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:23:57.977597 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:23:57.855966\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 992.97tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:23:58.043327 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:24:42.293871 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:24:42.298871 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 96000] type <class 'numpy.float32'> \n",
      "I0617 16:24:42.312871 16056 quant_utils.py:507] Random input name input shape [1, 96000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:24:42.315871 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:24:42.322872 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:24:42.329074 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:24:42.330074 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:24:42.370299 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:24:42.391182 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:24:42.412455 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:24:42.415458 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:24:42.416455 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:24:42.282870\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 583.22tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:24:42.474968 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:25:27.502602 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:25:27.506602 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 112000] type <class 'numpy.float32'> \n",
      "I0617 16:25:27.521603 16056 quant_utils.py:507] Random input name input shape [1, 112000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:25:27.524601 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:25:27.531601 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:25:27.535602 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:25:27.536602 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:25:27.648642 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:25:27.490600\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0617 16:25:27.671624 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:25:27.696015 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:25:27.699475 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:25:27.699475 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 400.47tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:25:27.775130 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:26:13.264246 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:26:13.269604 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 128000] type <class 'numpy.float32'> \n",
      "I0617 16:26:13.285644 16056 quant_utils.py:507] Random input name input shape [1, 128000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:26:13.289643 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:26:13.297354 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:26:13.301353 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:26:13.302351 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:26:13.336360 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:26:13.357780 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:26:13.379675 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:26:13.383881 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:26:13.384876 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:26:13.255968\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 409.42tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:26:13.459746 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:26:58.707214 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:26:58.711614 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 144000] type <class 'numpy.float32'> \n",
      "I0617 16:26:58.727613 16056 quant_utils.py:507] Random input name input shape [1, 144000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:26:58.732112 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:26:58.739166 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:26:58.743166 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:26:58.744608 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:26:58.780136 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:26:58.802272 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:26:58.823789 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:26:58.826795 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:26:58.826795 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:26:58.699216\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 389.73tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:26:58.892248 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:27:45.324639 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:27:45.330301 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 160000] type <class 'numpy.float32'> \n",
      "I0617 16:27:45.337665 16056 quant_utils.py:507] Random input name input shape [1, 160000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:27:45.337665 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:27:45.356637 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:27:45.363464 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:27:45.364350 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:27:45.396898 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:27:45.420271 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:27:45.444142 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:27:45.446529 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:27:45.448298 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:27:45.315274\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 234.07tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:27:45.537792 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:28:31.707780 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:28:31.711990 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 176000] type <class 'numpy.float32'> \n",
      "I0617 16:28:31.730989 16056 quant_utils.py:507] Random input name input shape [1, 176000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:28:31.735991 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:28:31.743314 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:28:31.747312 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:28:31.747312 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:28:31.779848 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:28:31.801153 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:28:31.825326 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:28:31.829699 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:28:31.830692 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:28:31.697814\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 179.49tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:28:31.923796 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:29:18.486829 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:29:18.491834 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 192000] type <class 'numpy.float32'> \n",
      "I0617 16:29:18.505832 16056 quant_utils.py:507] Random input name input shape [1, 192000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:29:18.508831 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:29:18.514830 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:29:18.520318 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:29:18.521274 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:29:18.553886 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:29:18.576632 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:29:18.597477 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:29:18.601482 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:29:18.602477 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:29:18.479829\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 213.76tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:29:18.684905 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:30:06.228460 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:30:06.232954 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 208000] type <class 'numpy.float32'> \n",
      "I0617 16:30:06.247952 16056 quant_utils.py:507] Random input name input shape [1, 208000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:30:06.251952 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:30:06.259954 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:30:06.263951 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:30:06.264952 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:30:06.295952 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:30:06.316953 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:30:06.338161 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:30:06.342410 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:30:06.342410 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:30:06.220451\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 189.17tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:30:06.429211 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:30:54.336942 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:30:54.341126 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 224000] type <class 'numpy.float32'> \n",
      "I0617 16:30:54.372411 16056 quant_utils.py:507] Random input name input shape [1, 224000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:30:54.375411 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:30:54.381850 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:30:54.386850 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:30:54.387850 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:30:54.430337 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:30:54.453016 16056 quantize.py:415] Start calibration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:30:54.328781\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:30:54.598870 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:30:54.602824 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:30:54.603329 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 148.13tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:30:54.748584 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:31:42.603422 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:31:42.626118 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 240000] type <class 'numpy.float32'> \n",
      "I0617 16:31:42.636985 16056 quant_utils.py:507] Random input name input shape [1, 240000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:31:42.636985 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:31:42.654647 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:31:42.661333 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:31:42.662332 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:31:42.698982 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:31:42.722350 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:31:42.741321 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:31:42.751940 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:31:42.752951 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:31:42.603423\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 174.44tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:31:42.852623 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:32:31.112707 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:32:31.116708 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 256000] type <class 'numpy.float32'> \n",
      "I0617 16:32:31.133970 16056 quant_utils.py:507] Random input name input shape [1, 256000] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:32:31.142968 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:32:31.166968 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:32:31.173551 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:32:31.174559 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:32:31.209155 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:32:31.232511 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:32:31.262661 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:32:31.269551 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:32:31.270547 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:32:31.099711\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 152.47tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:32:31.372815 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  25 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:33:20.011503 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:33:20.015506 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "I0617 16:33:20.034507 16056 quant_utils.py:507] Random input name input shape [1, 17920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:33:20.053505 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:33:20.059507 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:33:20.068504 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:33:20.069505 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:33:20.096935 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:33:20.118626 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:33:20.155193 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:33:20.156708 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:33:20.157706 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:33:20.003505\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.13tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:33:20.213709 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:34:08.819339 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:34:08.825340 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "I0617 16:34:08.842339 16056 quant_utils.py:507] Random input name input shape [1, 35840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:34:08.845846 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:34:08.851802 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:34:08.858438 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:34:08.858438 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:34:08.890822 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:34:08.911046 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:34:08.932434 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:34:08.933435 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:34:08.934438 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:34:08.812340\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1400.70tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:34:08.989460 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:34:57.870594 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:34:57.887012 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 53760] type <class 'numpy.float32'> \n",
      "I0617 16:34:57.891740 16056 quant_utils.py:507] Random input name input shape [1, 53760] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:34:57.902885 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:34:57.909336 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:34:57.914334 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:34:57.915333 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:34:57.945452 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:34:58.005661 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:34:58.029092 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:34:58.032514 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:34:58.032514 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:34:57.870594\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 801.66tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:34:58.095629 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:35:47.217526 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:35:47.222527 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "I0617 16:35:47.237598 16056 quant_utils.py:507] Random input name input shape [1, 71680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:35:47.239601 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:35:47.246990 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:35:47.254000 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:35:47.255000 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:35:47.284841 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:35:47.295962 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:35:47.314831 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:35:47.328126 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:35:47.328126 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:35:47.209528\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1933.24tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:35:47.392908 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:36:36.798081 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:36:36.798081 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 89600] type <class 'numpy.float32'> \n",
      "I0617 16:36:36.822733 16056 quant_utils.py:507] Random input name input shape [1, 89600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:36:36.823145 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:36:36.833687 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:36:36.837137 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:36:36.838162 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:36:36.869538 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:36:36.891764 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:36:36.914952 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:36:36.916950 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:36:36.918044 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:36:36.796015\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 535.29tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:36:36.994171 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:37:27.515955 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:37:27.522200 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 107520] type <class 'numpy.float32'> \n",
      "I0617 16:37:27.538131 16056 quant_utils.py:507] Random input name input shape [1, 107520] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:37:27.542443 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:37:27.549825 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:37:27.555620 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:37:27.556617 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:37:27.587979 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:37:27.609605 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:37:27.621107 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:37:27.633293 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:37:27.634301 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:37:27.500323\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 293.52tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:37:27.733531 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:38:18.138719 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:38:18.144229 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 125440] type <class 'numpy.float32'> \n",
      "I0617 16:38:18.160327 16056 quant_utils.py:507] Random input name input shape [1, 125440] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:38:18.187674 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:38:18.195739 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:38:18.201734 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:38:18.202737 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:38:18.232441 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:38:18.257035 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:38:18.278278 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:38:18.281360 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:38:18.282357 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:38:18.129749\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 396.73tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:38:18.364998 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:39:08.979122 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:39:08.987595 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 143360] type <class 'numpy.float32'> \n",
      "I0617 16:39:08.996836 16056 quant_utils.py:507] Random input name input shape [1, 143360] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:39:09.008280 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:39:09.014850 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:39:09.014850 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:39:09.023995 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:39:09.048871 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:39:09.075968 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:39:09.097080 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:39:09.097080 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:39:09.097080 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:39:08.963500\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 248.39tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:39:09.190791 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:40:00.472201 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:40:00.477579 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 161280] type <class 'numpy.float32'> \n",
      "I0617 16:40:00.491576 16056 quant_utils.py:507] Random input name input shape [1, 161280] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:40:00.495192 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:40:00.502977 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:40:00.507486 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:40:00.508491 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:40:00.539491 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:40:00.560909 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:40:00.580907 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:40:00.584911 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:40:00.585908 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:40:00.466201\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 233.34tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:40:00.669931 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:40:52.793519 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:40:52.797516 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 179200] type <class 'numpy.float32'> \n",
      "I0617 16:40:52.814841 16056 quant_utils.py:507] Random input name input shape [1, 179200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:40:52.817842 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:40:52.824844 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:40:52.829843 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:40:52.831646 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:40:52.864344 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:40:52.885428 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:40:52.906599 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:40:52.909601 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:40:52.910599 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:40:52.787515\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 205.74tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:40:52.999623 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:41:44.576215 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:41:44.582215 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 197120] type <class 'numpy.float32'> \n",
      "I0617 16:41:44.597419 16056 quant_utils.py:507] Random input name input shape [1, 197120] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:41:44.602587 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:41:44.610362 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:41:44.616801 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:41:44.618088 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:41:44.645828 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:41:44.666820 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:41:44.688814 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:41:44.692816 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:41:44.693817 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:41:44.566216\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 197.31tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:41:44.795317 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:42:37.517374 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:42:37.526739 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 215040] type <class 'numpy.float32'> \n",
      "I0617 16:42:37.531948 16056 quant_utils.py:507] Random input name input shape [1, 215040] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:42:37.548628 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:42:37.548628 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:42:37.561431 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:42:37.561431 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:42:37.597240 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:42:37.617044 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:42:37.645633 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:42:37.649634 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:42:37.649634 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:42:37.501748\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 175.83tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:42:37.756987 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:43:29.941592 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:43:29.961947 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 232960] type <class 'numpy.float32'> \n",
      "I0617 16:43:29.966884 16056 quant_utils.py:507] Random input name input shape [1, 232960] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:43:29.966884 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:43:29.983536 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:43:29.983536 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:43:29.983536 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:43:30.025707 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:43:30.040050 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:43:30.066779 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:43:30.068614 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:43:30.068614 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:43:29.941592\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 209.54tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:43:30.169376 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:44:22.862018 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:44:22.867020 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 250880] type <class 'numpy.float32'> \n",
      "I0617 16:44:22.882286 16056 quant_utils.py:507] Random input name input shape [1, 250880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:44:22.887286 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:44:22.894412 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:44:22.901412 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:44:22.902410 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:44:22.933879 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:44:22.954974 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:44:22.974972 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:44:22.980976 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:44:22.981972 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:44:22.855017\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 158.59tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:44:23.085384 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:45:16.161715 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:45:16.168186 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 268800] type <class 'numpy.float32'> \n",
      "I0617 16:45:16.192388 16056 quant_utils.py:507] Random input name input shape [1, 268800] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:45:16.196389 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:45:16.205802 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:45:16.213311 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:45:16.214712 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:45:16.242833 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:45:16.265142 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:45:16.288180 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:45:16.294300 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:45:16.295298 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:45:16.154646\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 87.51tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:45:16.444424 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:46:09.992331 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:46:10.013083 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 286720] type <class 'numpy.float32'> \n",
      "I0617 16:46:10.029402 16056 quant_utils.py:507] Random input name input shape [1, 286720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:46:10.029402 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:46:10.042110 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:46:10.046106 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:46:10.048191 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:46:10.090158 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:46:10.107689 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:46:10.123886 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:46:10.140182 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:46:10.140182 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:46:09.992331\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 89.68tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:46:10.286414 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  28 This is P:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:47:04.174081 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:47:04.178290 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 19840] type <class 'numpy.float32'> \n",
      "I0617 16:47:04.196290 16056 quant_utils.py:507] Random input name input shape [1, 19840] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:47:04.200289 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:47:04.205291 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:47:04.211288 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:47:04.212289 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:47:04.254298 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:47:04.274298 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:47:04.298302 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:47:04.299300 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:47:04.300300 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:47:04.166081\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1750.02tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:47:04.378301 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:47:58.056082 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:47:58.062082 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 39680] type <class 'numpy.float32'> \n",
      "I0617 16:47:58.076081 16056 quant_utils.py:507] Random input name input shape [1, 39680] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:47:58.080082 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:47:58.086462 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:47:58.092464 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:47:58.093844 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:47:58.126874 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:47:58.148865 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:47:58.172865 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:47:58.173943 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:47:58.174949 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:47:58.048079\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 944.39tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:47:58.271665 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:48:51.838398 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:48:51.857212 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 59520] type <class 'numpy.float32'> \n",
      "I0617 16:48:51.875020 16056 quant_utils.py:507] Random input name input shape [1, 59520] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:48:51.878020 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:48:51.884353 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:48:51.888843 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:48:51.889843 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:48:51.909989 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:48:51.932333 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:48:51.958358 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:48:51.960356 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:48:51.961357 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:48:51.838398\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:48:52.024162 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:49:46.435638 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:49:46.439638 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 79360] type <class 'numpy.float32'> \n",
      "I0617 16:49:46.456251 16056 quant_utils.py:507] Random input name input shape [1, 79360] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:49:46.475777 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:49:46.493775 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:49:46.499329 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:49:46.500615 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:49:46.528405 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:49:46.548871 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:49:46.572870 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:49:46.574872 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:49:46.575873 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:49:46.427638\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 538.48tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:49:46.656070 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:50:41.356538 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:50:41.362084 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 99200] type <class 'numpy.float32'> \n",
      "I0617 16:50:41.379602 16056 quant_utils.py:507] Random input name input shape [1, 99200] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:50:41.383778 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:50:41.391832 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:50:41.398235 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:50:41.399350 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:50:41.429138 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:50:41.449460 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:50:41.467653 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:50:41.473416 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:50:41.474420 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:50:41.352660\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 477.28tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:50:41.554098 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:51:36.599848 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:51:36.604849 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 119040] type <class 'numpy.float32'> \n",
      "I0617 16:51:36.619849 16056 quant_utils.py:507] Random input name input shape [1, 119040] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:51:36.625848 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:51:36.640878 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:51:36.645971 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:51:36.646969 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:51:36.686180 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:51:36.706180 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:51:36.729180 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:51:36.733182 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:51:36.733182 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:51:36.592854\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 437.38tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:51:36.821203 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:52:32.412448 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:52:32.416447 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 138880] type <class 'numpy.float32'> \n",
      "I0617 16:52:32.432447 16056 quant_utils.py:507] Random input name input shape [1, 138880] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:52:32.436722 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:52:32.443929 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:52:32.449927 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:52:32.450926 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:52:32.481675 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:52:32.502676 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:52:32.524801 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:52:32.527801 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:52:32.528802 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:52:32.402445\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 368.36tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:52:32.620314 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:53:29.196883 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:53:29.202325 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 158720] type <class 'numpy.float32'> \n",
      "I0617 16:53:29.216324 16056 quant_utils.py:507] Random input name input shape [1, 158720] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:53:29.219324 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:53:29.224526 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:53:29.228526 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:53:29.229528 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:53:29.258064 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:53:29.286326 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:53:29.308326 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:53:29.311332 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:53:29.312334 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:53:29.190511\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 218.77tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:53:29.413349 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:54:25.641462 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:54:25.646565 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 178560] type <class 'numpy.float32'> \n",
      "I0617 16:54:25.663481 16056 quant_utils.py:507] Random input name input shape [1, 178560] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:54:25.667482 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:54:25.672010 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:54:25.681162 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:54:25.682162 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:54:25.712692 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:54:25.728780 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:54:25.748315 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:54:25.748315 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:54:25.748315 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:54:25.625369\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 277.15tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:54:25.846074 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:55:22.283783 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:55:22.288783 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 198400] type <class 'numpy.float32'> \n",
      "I0617 16:55:22.303782 16056 quant_utils.py:507] Random input name input shape [1, 198400] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:55:22.307784 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:55:22.315903 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:55:22.324414 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:55:22.325415 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:55:22.355416 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:55:22.382177 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:55:22.403843 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:55:22.407847 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:55:22.408845 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:55:22.277782\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 200.01tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:55:22.514869 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:56:19.046064 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:56:19.050065 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 218240] type <class 'numpy.float32'> \n",
      "I0617 16:56:19.066066 16056 quant_utils.py:507] Random input name input shape [1, 218240] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:56:19.071064 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:56:19.079068 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:56:19.087074 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:56:19.088078 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:56:19.117415 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:56:19.137174 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:56:19.159438 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:56:19.162741 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:56:19.163741 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:56:19.040064\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 178.02tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:56:19.289811 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:57:16.566132 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:57:16.571260 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 238080] type <class 'numpy.float32'> \n",
      "I0617 16:57:16.596777 16056 quant_utils.py:507] Random input name input shape [1, 238080] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:57:16.600314 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:57:16.609322 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:57:16.615322 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:57:16.616320 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:57:16.644555 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:57:16.668556 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:57:16.692557 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:57:16.698028 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:57:16.699024 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:57:16.557997\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 145.83tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:57:16.820048 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:58:14.192686 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:58:14.197154 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 257920] type <class 'numpy.float32'> \n",
      "I0617 16:58:14.216151 16056 quant_utils.py:507] Random input name input shape [1, 257920] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:58:14.219153 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:58:14.226338 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:58:14.231335 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:58:14.232334 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:58:14.264963 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:58:14.285967 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:58:14.305963 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:58:14.311256 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:58:14.312253 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:58:14.184687\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 162.52tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:58:14.424349 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 16:59:13.172142 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 16:59:13.190401 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 277760] type <class 'numpy.float32'> \n",
      "I0617 16:59:13.198127 16056 quant_utils.py:507] Random input name input shape [1, 277760] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 16:59:13.216750 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 16:59:13.225189 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 16:59:13.229187 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 16:59:13.229187 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 16:59:13.264709 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 16:59:13.285423 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 16:59:13.309434 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 16:59:13.315435 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 16:59:13.316432 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 16:59:13.172142\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75.52tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 16:59:13.480961 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 17:00:12.059901 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 17:00:12.076338 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 297600] type <class 'numpy.float32'> \n",
      "I0617 17:00:12.082220 16056 quant_utils.py:507] Random input name input shape [1, 297600] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 17:00:12.093846 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 17:00:12.093846 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 17:00:12.109473 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 17:00:12.110448 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 17:00:12.181296 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 17:00:12.198652 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 17:00:12.228338 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 17:00:12.059901\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0617 17:00:12.236336 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 17:00:12.237335 16056 calibrate.py:371] Use all calibration data to calculate min mse\n",
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 67.93tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 17:00:12.425868 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quantize:calibration_data_reader is None, using random data for calibration\n",
      "I0617 17:01:11.610861 16056 quantize.py:164] calibration_data_reader is None, using random data for calibration\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "I0617 17:01:11.615987 16056 quant_utils.py:1232] The input ONNX model models/pfb.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quant_utils:Random input name input shape [1, 317440] type <class 'numpy.float32'> \n",
      "I0617 17:01:11.637062 16056 quant_utils.py:507] Random input name input shape [1, 317440] type <class 'numpy.float32'> \n",
      "INFO:vai_q_onnx.quant_utils:Obtained calibration data with 1 iters\n",
      "I0617 17:01:11.642023 16056 quant_utils.py:293] Obtained calibration data with 1 iters\n",
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/pfb.onnx can run inference successfully\n",
      "I0617 17:01:11.651111 16056 quant_utils.py:1211] The input ONNX model models/pfb.onnx can run inference successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "I0617 17:01:11.657268 16056 quantize.py:209] Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "I0617 17:01:11.658648 16056 quantize.py:213] Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "I0617 17:01:11.691060 16056 quantize.py:340] enable_ipu_cnn is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "I0617 17:01:11.712503 16056 quantize.py:415] Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "I0617 17:01:11.723674 16056 quantize.py:429] Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "I0617 17:01:11.741492 16056 calibrate.py:299] Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n",
      "I0617 17:01:11.741492 16056 calibrate.py:371] Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-06-17 17:01:11.596209\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- ChrisMiniPC\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22631\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.19\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+69bc4f2\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/pfb.onnx\n",
      "                                  model_output --- models/pfb_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_ipu_cnn --- True\n",
      "                        enable_ipu_transformer --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 51.14tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n",
      "I0617 17:01:11.971585 16056 qdq_quantizer.py:886] Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Constant             │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 14                        </span>│\n",
       "│ Gather               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Unsqueeze            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Concat               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Reshape              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "│ Transpose            │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Slice                │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/pfb_quantized.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Shape                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Constant             │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m14                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gather               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Unsqueeze            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Concat               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Reshape              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Transpose            │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Slice                │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/pfb_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M:  31 This is P:  1024\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "M_increment = 3\n",
    "P_increment = 64\n",
    "Min_M = 1\n",
    "Min_P = 64\n",
    "Max_M = 31\n",
    "Max_P = 1024\n",
    "batchsize = 1\n",
    "Maxloop = 30\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "for M in range(Min_M, Max_M + 1, M_increment):\n",
    "    for P in range(Min_P, Max_P + 1, P_increment):\n",
    "        createmodel(M, P)\n",
    "        #print(\"This is M again: \", M)\n",
    "        print(\"This is M: \", M, \"This is P: \", P)\n",
    "        x = np.sin((np.arange(0, M * P * 10) / np.pi),  dtype=np.float32)\n",
    "        win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "\n",
    "        # Converting NumPy arrays to CuPy arrays\n",
    "        \n",
    "        elapsed_time_FFT = 100\n",
    "        elapsed_time_TINA_FFT = 100\n",
    "        bytes_x = x.nbytes\n",
    "        coef_bytes = win_coeffs.nbytes\n",
    "        \n",
    "        for i in range(Maxloop):\n",
    "            x = np.random.rand(*x.shape).astype(np.float32)\n",
    "\n",
    "            input_data = np.random.uniform(low=-1, high=1, size=[1,M * P * 10]).astype(np.float32)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "            # Timing pfb_fir_frontend_FFT\n",
    "           \n",
    "            intermediatetime = benchmarkCPU( win_coeffs = win_coeffs, M = M, P = P)\n",
    "            \n",
    "            elapsed_time_FFT = comparedspeedup(elapsed_time_FFT, intermediatetime)\n",
    "            #print(\"elapsed_time_FFT = \" , elapsed_time_FFT)\n",
    "                        \n",
    "\n",
    "           \n",
    "            # Timing pfb_fir_frontend_TINA_FFT\n",
    "            xinput = np.random.rand(batchsize,*x.shape)\n",
    "            elapsed_time_TINA_FFT_temp = runIPU(input_data = input_data)\n",
    "            elapsed_time_TINA_FFT = comparedspeedup(elapsed_time_TINA_FFT, elapsed_time_TINA_FFT_temp)\n",
    "            #print(\"elapsed_time_TINA_FFT = \" , elapsed_time_TINA_FFT)\n",
    "            \n",
    "\n",
    "        # Average the elapsed times\n",
    "        \"\"\"\n",
    "        elapsed_time /= Maxloop\n",
    "        elapsed_time_FFT /= Maxloop\n",
    "        elapsed_time_cp /= Maxloop\n",
    "        elapsed_time_cp_FFT /= Maxloop\n",
    "        elapsed_time_TINA /= Maxloop\n",
    "        elapsed_time_TINA_FFT /= Maxloop\n",
    "        elapsed_time_jax /= Maxloop\n",
    "        elapsed_time_jax_fft /= Maxloop\n",
    "        \"\"\"\n",
    "\n",
    "         # Calculate speedup for CuPy, Torch, and JAX compared to NumPy\n",
    "        speedup_TINA_FFT = elapsed_time_FFT / elapsed_time_TINA_FFT\n",
    "        \n",
    "        #print(\"TINA: \", elapsed_time_TINA_FFT)\n",
    "        #print(\"cupy: \", elapsed_time_cp_FFT)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'Taps': M,\n",
    "            'Branches': P,\n",
    "            'bytes_x': bytes_x,\n",
    "            'coef_bytes': coef_bytes,\n",
    "            'elapsed_time_FFT_CPU': elapsed_time_FFT,\n",
    "            'elapsed_time_TINA_FFT': elapsed_time_TINA_FFT,\n",
    "            'speedup_TINA_FFT':speedup_TINA_FFT,\n",
    "            \n",
    "\n",
    "        })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Divide the values by Maxloop\n",
    "#df[['elapsed_time', 'elapsed_time_FFT', 'elapsed_time_cp', 'elapsed_time_cp_FFT', 'elapsed_time_TINA', 'elapsed_time_TINA_FFT']] /= Maxloop\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('output PFB.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define model class\n",
    "class SmallModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Instantiate model and generate inputs\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "pytorch_model = SmallModel(input_size, output_size)\n",
    "\n",
    "print(pytorch_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The following code is used for exporting a PyTorch model (pytorch_model) to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the VitisAI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Prep for ONNX export\\ninputs = {\"x\": torch.rand(input_size, input_size)}\\ninput_names = [\\'input\\']\\noutput_names = [\\'output\\']\\ndynamic_axes = {\\'input\\': {0: \\'batch_size\\'}, \\'output\\': {0: \\'batch_size\\'}}\\ntmp_model_path = \"models/pfb.onnx\"\\n\\n# Call export function\\ntorch.onnx.export(\\n        pytorch_model,\\n        inputs,\\n        tmp_model_path,\\n        export_params=True,\\n        opset_version=13,  # Recommended opset\\n        input_names=input_names,\\n        output_names=output_names,\\n        dynamic_axes=dynamic_axes,\\n    )\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Prep for ONNX export\n",
    "inputs = {\"x\": torch.rand(input_size, input_size)}\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = \"models/pfb.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        pytorch_model,\n",
    "        inputs,\n",
    "        tmp_model_path,\n",
    "        export_paradef createmodel(M, P):\n",
    "    x = np.sin(np.arange(0, M * P * 10) / np.pi)\n",
    "    win_coeffs = generate_win_coeffs(M, P, window_fn=\"hamming\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.random.rand(*x.shape).astype(np.float32)\n",
    "    \n",
    "    win_coeffs = torch.from_numpy(win_coeffs)\n",
    "    \n",
    "    # Timing pfb_fir_frontend_TINA_FFT\n",
    "    xinput = np.random.rand(batchsize,*x.shape)\n",
    "    PFB_layer = PFB_FIR_FFT(win_coeffs = win_coeffs, M = M, P = P, expected_input_size=xinput.shape[1])\n",
    "    PFB_layer = PFB_layer.float()\n",
    "    xinput = torch.from_numpy(xinput).float()\n",
    "    tmp_model_path = \"models/pfb.onnx\"\n",
    "    torch.onnx.export(\n",
    "    PFB_layer,                     # model being run\n",
    "    xinput,                 # model input (or a tuple for multiple inputs)\n",
    "    tmp_model_path,                     # where to save the model\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=13,              # the ONNX version to export the model to\n",
    "    input_names=['input'],         # the model's input names\n",
    "    output_names=['output'],       # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # variable length axes\n",
    "    )\n",
    "\n",
    "    # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "    input_model_path = \"models/pfb.onnx\"\n",
    "    \n",
    "    # `output_model_path` is the path where the quantized model will be saved.\n",
    "    output_model_path = \"models/pfb_quantized.onnx\"\n",
    "    \n",
    "    vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    calibration_data_reader=None,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True,\n",
    "    extra_options={'ActivationSymmetric': True}\n",
    "    )\n",
    "    \n",
    "ms=True,\n",
    "        opset_version=13,  # Recommended opset\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the Vitis AI Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. For more information on this quantization method, see [Vitis AI ONNX Quantization](https://ryzenai.docs.amd.com/en/latest/vai_quant/vai_q_onnx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Model\n",
    "\n",
    "#### CPU Run\n",
    "\n",
    "Before runnning the model on the IPU, we'll run the model on the CPU and get the execution time for comparison with the IPU. We'll also use the ONNX Runtime Profiling to get some more information about the inference. For more information on this, see [Profiling Tools](https://onnxruntime.ai/docs/performance/tune-performance/profiling-tools.html) from ONNX Runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPU Run\n",
    "\n",
    "Now, we'll run it on the IPU and time the execution so that we can compare the results with the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather our results and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
